---
title: "Monte Carlo Power Analysis: Independent Two Sample Difference of Means"
description: |
  A short description of the post.
author:
  - name: Gavin Gordon
    url: https://example.com/norajones
date: 10-25-2021
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r message=FALSE,warning=F,echo=FALSE}
library(tidyverse)
library(gganimate)
library(plotly)
library(sjPlot)
library(brms)
library(pander)
```

# Motivation

I found it discouraging that a power analysis for a multinomial multiple logistic regression wasn't readily accessible/available in the same vein as one could use the technique. Eventually I started to grasp why it might be difficult to distill the complexity of a dataset to a few much less one formula for a sample size.

This led me to favor simulations. Okay everything in stats is a simulation but that itself wasn't evident to me for quite some time until I started diving into the Bayesian methods with respect to posterior predictive checks.

This is first a proof of exercise, hopefully one that will generalize to any form of modeling exercise one could undertake for determining sample sizes/power as well as add to my capacity for performing simulations. As modeling becomes more complex such as with hierarchical models, simulations become inevitable so this is also me setting the ground work.

A Power/Sample Size analysis forces you to become intimate with your expectations from data, to consider many possible analyses, while also serving as management tool for providing information for cost analysis and interim analysis (such as for clinical trials which I hope to make a general use case of). Simulations extend this capability by allowing you to include many additional variables while being able to vary the relationships you between these variables. You would not have to depend on sourcing prepackaged code for power/sample size calculations when considering different analysis as you are in full control of the data generating mechanism; you only need what may already be available to you in terms of the models you expect to run.... and also hardware specs that won't deplete your patience.

## Intuition behind the technique

Basically you create datasets, and perform the analysis you expect to do. This data generation process is guided by the knowledge sourced from your literature review so it is imperative you examine the results in detail to understand what kinds of datasets you may encounter.

You simulate thousands of datasets and check how well a particular sample size will capture an effect you're either interested in or one you think exists.

```{r echo=F,eval=T}

build_sample <- function(
  nsims,
  u1,
  effect_size_u,
  allocate_n1,
  allocate_n2,
  sample_size,
  sigma_u1,
  effect_size_sigma
                         ){
  
  # get the sample size right for the two groups while respecting the allocations 
  number_batches = floor(sample_size/(allocate_n1 + allocate_n2))
  n1 = number_batches * allocate_n1
  n2 = number_batches * allocate_n2
  total_sample_size = n1 + n2
  
  
  # create the coefficient matrix
  treatment_vector = c(rep(0,n1),
                       rep(1,n2))
  
  intercept_vector = rep(1,total_sample_size)
  
  x_matrix <-   matrix(c(intercept_vector,
                         treatment_vector),
                       nrow = total_sample_size)
  
  
  # create the mean regression vector
  coeff_mean_vector <- matrix(c(u1,effect_size_u))
  u_regression <- x_matrix %*% coeff_mean_vector
  
  
  # create the variance regression vector
  coeff_sigma_vector <- matrix(c(sigma_u1,effect_size_sigma))
  sigma_regression <- x_matrix %*% coeff_sigma_vector
  
  
  y <- rnorm(total_sample_size,
             mean = u_regression,
             sd = sigma_regression)
  
  x <- c(rep("group one",n1),rep("group two",n2))
  
  
  df <- data.frame(x = factor(x),
                   y = y)
  # hist(c(y1,y2))
  
  
  return(df)
}


dataframes <- lapply(1:50,
                     build_sample,
                     u1 = 2,
                     effect_size_u = 3,
                     allocate_n1 = 1,
                     allocate_n2 = 1,
                     sample_size = 50,
                     sigma_u1 = 1,
                     effect_size_sigma = 0
                     )

dataframes <- bind_rows(dataframes, .id = "column_label")

ggplot(dataframes, aes(fill = factor(x), y)) + 
  geom_histogram() + 
  geom_vline(xintercept = 2,
             color = 'red') + 
  geom_vline(xintercept = 5,
             color = 'blue') + 
  theme_bw() + 
  labs(
    title = "Example Simulations with Location Effect Size of Three",
    x = "Y values",
    y = "Count",
    fill = "Groups"
  ) + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  # Here comes the gganimate code
  transition_states(
    column_label,
    transition_length = 7,
    state_length = 5
  ) +
  enter_fade() + 
  exit_shrink() +
  ease_aes('sine-in-out')
```

The above animation should give an easy enough view of data generation. The data is that from two Normal Distributions that have the same variance but their mode of central tendency are typically three units apart. Now that we are able to simulate datasets, now comes the application of whatever analysis you have in mind. For our purposes a regression approach is used (brms package in R) utilizing the defaults for Bayesian estimation.

For these problems I make use of the ability to explicitly model the variances instead of using a pooled estimate of some kind (heterogeneous regression modeling). Even though I know that the variances are the same for the above animations, this will set the trend for the upcoming problems for dealing with unequal variances.

An example of the model output is as such:

<center>

```{r echo = F}

regress <-  readRDS("C:/Users/gavin/Documents/GitHub/myBlog/regression_sample_p1.rds")

tab_model(regress)
```

</center>

We have two intercepts and two effect sizes. This is because there is one regression equation that models the mean of the normal distribution, and similarly there is another regression that models the variance of the normal distribution.

The variances are given on a log scale and must be exponentiated before we can interpret on the same scale as our data as shown below

<center>

```{r echo=F}

hyp <- c("exp(sigma_Intercept) = 0",
         "exp(sigma_Intercept + sigma_xgrouptwo) = 0")
pander(hypothesis(regress, hyp)$hypothesis[,2:5])
```

</center>

The relevant information is extracted (mostly the quantiles of the posterior distribution of parameters) and then the entire process is repeated several times for any one sample size and also in our case for different group allocation schemes.

I am interested in the following:

1.  Have we managed to recover the parameters used to generate the sample? Making sure you check for all the parameters becomes important such as in situations when you expect to have unequal variances between groups.
2.  Is the length of the credible interval small enough? This is something I would discuss with a client. How much precision around the estimate would there have to be before the estimate becomes action worthy?
3.  Is worth considering an unequal treatment/sampling allocation (something other than a 1:1 ratio)?
4.  Is the credible interval typically positioned away from values which I would consider there to be no effect?

## On to the Simulations

### Problem One

The first problem we will consider is to determine a sample size that can recover the location and scale effects given below with a credible interval length of 0.7 for the precision. The difference in means is 0.5 magnitude with the variance between the two groups being the same but over varying group allocations (1-1,1-3,1-5).

$$
\mu_1 \sim N(2,1) 
$$

$$
\mu_2 \sim N(2.5,1)
$$

$$
e \sim N(0,1)
$$

$$
sigma ~ \gamma()
$$

The first useful piece of information is that for the estimated difference in the means.

<center>

```{r echo = F,fig.width= 7}
info <- readRDS("C:/Users/gavin/Documents/GitHub/myBlog/documents/two_sample_power_files/all_problems.rds")  %>%
  filter(para_com == "2 - 0.5 - 1 - 0")

pop_u = 2
pop_effect_size_u = 0.5 
pop_sigma_u1 = 1
pop_effect_size_sigma = 0


grouptwo_info <- info %>%
  filter(para_com == "2 - 0.5 - 1 - 0") %>% 
  ggplot(aes(x = sample_size,
             y = xgrouptwo,
             color = allocation,
             ymin = xgrouptwo_q2.5,
             ymax = xgrouptwo_q97.5,
             text = paste('interval length: ', xgrouptwo_length,
                          '</br>interval length criteria satisfied: ', xgrouptwo_cover,
                          '</br>detect probability: ', xgrouptwo_detect,
                          '</br>group one sample size: ',n1,
                          '</br>group two sample size: ',n2
             ))) +
  geom_hline(yintercept = c(0, .5), color = "white") +
  geom_pointrange(fatten = 1/2) +
  # scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) + 
  geom_point() + 
  ylim(min(info$xgrouptwo_q2.5) - 0.5,max(info$xgrouptwo_q97.5) + 0.5) + 
  geom_hline(yintercept = 0.5) + 
  labs(x = "Sample Size (Total)",
       y = "Location Effect Size",
       title = "Properties of the Location Effect across Sample Sizes") + 
  theme_bw() 


ggplotly(grouptwo_info) %>% 
  layout(hovermode = "x")


```

</center>

By hovering over a particular point we have access to more information about that particular sample size.

Almost all samples shown have the ability to include the location effect size a minimum of `r min(info$xgrouptwo_detect)` proportion of the time across all allocations. It seems that you don't quite need to worry about detecting the effect though the uncertainty around the estimates can be so terrible as to make the estimate untrustworthy as can be seen by the wide credible intervals for the smaller sample sizes.

This is why we directly consider the length of the intervals. The criteria set for this simulation was that of 0.7 for the credible interval; the label of xgrouptwo_cover will show the proportion of times this criteria was satisfied. The 1-1 allocation first met this requirement for sample sizes `r info %>% filter(allocation == '1-1',xgrouptwo_length < 0.7) %>% dplyr::select(sample_size) %>% min()` but was not able to maintain this for 0.8 proportion of the times until the sample size was `r info %>% filter(allocation == '1-1',xgrouptwo_cover >= 0.8) %>% dplyr::select(sample_size) %>% min()` . For the 1-3 allocation these sample sizes were `r info %>% filter(allocation == '1-3',xgrouptwo_length < 0.7) %>% dplyr::select(sample_size) %>% min()` and `r info %>% filter(allocation == '1-3',xgrouptwo_cover >= 0.8) %>% dplyr::select(sample_size) %>% min()` respectively. For the 1-5 allocation the sample sizes were `r info %>% filter(allocation == '1-5',xgrouptwo_length < 0.7) %>% dplyr::select(sample_size) %>% min()` and `r info %>% filter(allocation == '1-5',xgrouptwo_cover >= 0.8) %>% dplyr::select(sample_size) %>% min()` respectively. Even though the length of the interval may be precise, what if it actually tends to occurs near values for which we dismiss the effect size as non-existent. This may not be of concern for larger effect sizes say those greater than 1, but as what you are trying to detect becomes smaller, the proximity to the zero value can become concerning. Of course bayesian estimation gives the probability of an interval around the zero value so this alleviates the concern to some extent. Even so this is a problem I've noticed with some power analysis in that it gives the minimum sample size needed to reject the Null Hypothesis, for which in this case it would be the sample size needed to exclude zero from the interval. The problem is that zero isn't exactly that far from the interval when using that sample size, thus you would have to be extremely precise in your prediction of the true variability if you were to take that sample size at face value.

Therefore one might also consider for what sample size does the lower bound of the credible interval tend to be above a constant for example 0.1; for the 1-1 allocation this first occurs at `r info %>% filter(allocation == '1-1', xgrouptwo_q2.5 > 0.1) %>% dplyr::select(sample_size) %>% min()` and it was not able to maintain this for 80% of the times until a sample size of `r info %>% filter(allocation == '1-1', xgrouptwo_q2.5_0.1above >= 0.8) %>% dplyr::select(sample_size) %>% min()` ;for 1-3 allocation this happened at sample sizes `r info %>% filter(allocation == '1-3', xgrouptwo_q2.5 > 0.1) %>% dplyr::select(sample_size) %>% min()` and `r info %>% filter(allocation == '1-3', xgrouptwo_q2.5_0.1above >= 0.8) %>% dplyr::select(sample_size) %>% min()` respectively; for 1-5 allocation at `r info %>% filter(allocation == '1-5', xgrouptwo_q2.5 > 0.1) %>% dplyr::select(sample_size) %>% min()` and `r info %>% filter(allocation == '1-5', xgrouptwo_q2.5_0.1above >= 0.8) %>% dplyr::select(sample_size) %>% min()` respectively.

The next informative parameter is that of the scale effect. It is important that this is captured well as this will decide whether your data is too noisy to even begin to interpret what you gathered. Remember that for this situation the variances are the same so we expect this scale effect parameter to be zero.

```{r echo=FALSE}

grouptwo_sd_info <- info %>%
  filter(para_com == "2 - 0.5 - 1 - 0") %>% 
  ggplot(aes(x = sample_size,
             y = sigma_xgrouptwo,
             color = allocation,
             ymin = sigma_xgrouptwo_q2.5,
             ymax = sigma_xgrouptwo_q97.5,
             text = paste('interval length: ', sigma_xgrouptwo_length,
                          '</br>interval length criteria satisfied: ', sigma_xgrouptwo_cover,
                          '</br>detect probability: ', sigma_xgrouptwo_detect,
                          '</br>group one sample size: ',n1,
                          '</br>group two sample size: ',n2
             ))) +
  geom_hline(yintercept = c(0, .5), color = "white") +
  geom_pointrange(fatten = 1/2) +
  # scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) + 
  geom_point() + 
  ylim(min(info$sigma_xgrouptwo_q2.5) - 0.5,max(info$sigma_xgrouptwo_q97.5) + 0.5) + 
  geom_hline(yintercept = 0) + 
  labs(x = "Sample Size (Total)",
       y = "Location Effect Size",
       title = "Properties of the Scale Effect across Sample Sizes") + 
  theme_bw() 


ggplotly(grouptwo_sd_info) %>% 
  layout(hovermode = "x")


```

All samples shown have the ability to include the scaleeffect size a minimum of `r min(info$sigma_xgrouptwo_detect)` proportion of the time across all allocations.

The length of the intervals. The criteria set for this simulation was that of 0.7 for the credible interval; the label of sigma_xgrouptwo_cover will show the proportion of times this criteria was satisfied. The 1-1,1-3 and 1-5 allocations met this requirement for sample sizes `r info %>% filter(allocation == '1-1',sigma_xgrouptwo_length < 0.7) %>% dplyr::select(sample_size) %>% min()` , `r info %>% filter(allocation == '1-3',sigma_xgrouptwo_length < 0.7) %>% dplyr::select(sample_size) %>% min()` , and `r info %>% filter(allocation == '1-5',sigma_xgrouptwo_length < 0.7) %>% dplyr::select(sample_size) %>% min()` respectively.

Since this estimate is centered around zero, I let the length of the interval serve as the limit for upper and lower bounds of the estimate.

Thus in summary the following table details for each allocation the sample size requirements if we want the following to be satisfied at least 80% of the time:

1.  location effect to be captured.

2.  location effect credible interval to have a length less than 0.7

3.  location effect credible interval lower bound to be above 0.1

4.  scale effect credible interval to have a length of 0.7

```{r}
allo_1_total_samplesize <- info %>% filter(allocation == '1-1',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(sample_size) %>% min()

allo_1_n1_samplesize <- info %>% filter(allocation == '1-1',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(n1) %>% min()

allo_1_n2_samplesize <- info %>% filter(allocation == '1-1',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(n2) %>% min()


# sample sizes for 1-3 allocation
allo_3_total_samplesize <- info %>% filter(allocation == '1-3',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(sample_size) %>% min()

allo_3_n1_samplesize <- info %>% filter(allocation == '1-3',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(n1) %>% min()

allo_3_n2_samplesize <- info %>% filter(allocation == '1-3',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(n2) %>% min()


# sample size for 1-5 allocation 
allo_5_total_samplesize <- info %>% filter(allocation == '1-5',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(sample_size) %>% min()

allo_5_n1_samplesize <- info %>% filter(allocation == '1-5',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(n1) %>% min()

allo_5_n2_samplesize <- info %>% filter(allocation == '1-5',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(n2) %>% min()


sample_size_tab <- data.frame(
  allocation = c(
    "1-1",
    "1-3",
    "1-5"
  ),
  total = c(
    allo_1_total_samplesize,
    allo_3_total_samplesize,
    allo_5_total_samplesize
    ),
  n1 = c(
    allo_1_n1_samplesize,
    allo_3_n1_samplesize,
    allo_5_n1_samplesize
  ),
  n2 = c(
    allo_1_n2_samplesize,
    allo_3_n2_samplesize,
    allo_5_n2_samplesize
  )
)

names(sample_size_tab) <- c(
  "Allocation",
  "Total Sample Size",
  "Group One Sample Size",
  "Group Two Sample Size"
)


pander(sample_size_tab)

```

### Problem Two

Now we will consider a problem that deals with a larger location effect size but with the complication of unequal variances.

$$
\mu_1 \sim N(2,1) 
$$

$$
\mu_2 \sim N(2.5,2)
$$

$$
e \sim N(0,1)
$$

For this problem we will not be calculating the sample size requirements as you will see it would require simulations for total sample sizes that exceed 400. Instead we will compare how the gains over the allocations compare to each other across the different criteria.

The first criteria of selection will be the proportion of times we were able to detect the estimate. You may recall this went well for the first problem and there does not seem to be a poor performance in the plot below.

<center>

```{r echo = F,fig.width= 7}
info <- readRDS("C:/Users/gavin/Documents/GitHub/myBlog/documents/two_sample_power_files/all_problems.rds")  %>%
  filter(para_com == "2 - 0.5 - 1 - 2")



grouptwo_info <- info %>%
  ggplot(aes(x = sample_size,
             y = xgrouptwo_detect,
             color = allocation,
             group = allocation,
             # ymin = xgrouptwo_q2.5,
             # ymax = xgrouptwo_q97.5,
             text = paste(
                          '</br>group one sample size: ',n1,
                          '</br>group two sample size: ',n2
             ))) +
  # geom_hline(yintercept = c(0, .5), color = "white") +
  # geom_pointrange(fatten = 1/2) +
  # scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) + 
  # geom_point() +
  geom_line() + 
  # ylim(min(info$xgrouptwo_q2.5) - 0.5,max(info$xgrouptwo_q97.5) + 0.5) + 
  # geom_hline(yintercept = 0.5) + 
  labs(x = "Sample Size (Total)",
       y = "Proportion",
       title = "Properties of the Location Detect Proportions across Allocations") + 
  theme_bw() 


ggplotly(grouptwo_info) %>% 
  layout(hovermode = "x")


```

</center>

Not sure what to comment right now about this. Going to wait until I am able to run more simulations.

The next criteria is the length of the intervals. Immediately we can see that the 1-1 allocation is always performing worse for all sample sizes relative to the other allocations.

```{r echo=FALSE}



grouptwo_info <- info %>%
  ggplot(aes(x = sample_size,
             y = xgrouptwo_length,
             color = allocation,
             group = allocation,
             # ymin = xgrouptwo_q2.5,
             # ymax = xgrouptwo_q97.5,
             text = paste(
                          '</br>group one sample size: ',n1,
                          '</br>group two sample size: ',n2
             ))) +
  # geom_hline(yintercept = c(0, .5), color = "white") +
  # geom_pointrange(fatten = 1/2) +
  # scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) + 
  # geom_point() +
  geom_line() + 
  # ylim(min(info$xgrouptwo_q2.5) - 0.5,max(info$xgrouptwo_q97.5) + 0.5) + 
  # geom_hline(yintercept = 0.5) + 
  labs(x = "Sample Size (Total)",
       y = "Proportion",
       title = "Properties of the Location Effect Interval Length across Allocations") + 
  theme_bw() 


ggplotly(grouptwo_info) %>% 
  layout(hovermode = "x")


```

The 1-3 allocation seems to be performing slightly better than the 1-5 allocation across all sample sizes. This difference seems to decrease as the sample sizes increase. Overall it would be better to use the 1-3 allocation. Interestingly the variance for group two is three times that of group one and so the allocation that is based on the ratio of variability works best.

```{r}



grouptwo_info <- info %>%
  ggplot(aes(x = sample_size,
             y = xgrouptwo_q2.5_0.1above,
             color = allocation,
             group = allocation,
             # ymin = xgrouptwo_q2.5,
             # ymax = xgrouptwo_q97.5,
             text = paste(
                          '</br>group one sample size: ',n1,
                          '</br>group two sample size: ',n2
             ))) +
  # geom_hline(yintercept = c(0, .5), color = "white") +
  # geom_pointrange(fatten = 1/2) +
  # scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) + 
  # geom_point() +
  geom_line() + 
  # ylim(min(info$xgrouptwo_q2.5) - 0.5,max(info$xgrouptwo_q97.5) + 0.5) + 
  # geom_hline(yintercept = 0.5) + 
  labs(x = "Sample Size (Total)",
       y = "Proportion",
       title = "Properties of the Location Detect Proportions across Allocations") + 
  theme_bw() 


ggplotly(grouptwo_info) %>% 
  layout(hovermode = "x")

```

Although I will wait for more simulations to be run, it does seem like the 1-3 allocation seems to be higher in the proportion than most sample sizes when compared to the 1-1 allocation. It is not clear that what the pattern is for the 1-3 compared to the 1-5 allocation as these seem to have similar performance.

```{r}



grouptwo_info <- info %>%
  ggplot(aes(x = sample_size,
             y = sigma_xgrouptwo_length,
             color = allocation,
             group = allocation,
             # ymin = xgrouptwo_q2.5,
             # ymax = xgrouptwo_q97.5,
             text = paste(
                          '</br>group one sample size: ',n1,
                          '</br>group two sample size: ',n2
             ))) +
  # geom_hline(yintercept = c(0, .5), color = "white") +
  # geom_pointrange(fatten = 1/2) +
  # scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) + 
  # geom_point() +
  geom_line() + 
  # ylim(min(info$xgrouptwo_q2.5) - 0.5,max(info$xgrouptwo_q97.5) + 0.5) + 
  # geom_hline(yintercept = 0.5) + 
  labs(x = "Sample Size (Total)",
       y = "Proportion",
       title = "Properties of the Scale Effect Interval Length across Allocations") + 
  theme_bw() 


ggplotly(grouptwo_info) %>% 
  layout(hovermode = "x")
```

Here we see the same pattern as with the Location effect interval where the 1-1 allocation performed worse compared to the other allocations, and of the remainder the 1-3 allocation performed better.

The question we may have is whether the difference in interval length is enough to consider using one allocation to the other.

```{r}


diff_allocation <- function(
  dataset,
  base_allocation,
  other_allocation,
  criteria,
  crit_name
  ){
  
   to_diff <- info %>% 
    filter(allocation == base_allocation) %>% 
    dplyr::select(
      criteria,
      sample_size
      )
  
  names(to_diff) <- c('first','sample_size')

  to_diff$second <- info %>% 
    filter(allocation == other_allocation) %>% 
    dplyr::pull(criteria)
  
  
  to_diff <- to_diff %>% 
    mutate(
      diff = first - second
    )
  
  
  p <- to_diff %>% 
  ggplot(
    aes(
      sample_size,
      diff)
    ) + 
    geom_line() + 
    labs(
      x = "Sample Size",
      y = "Difference",
      title = paste('Difference between',crit_name,"for allocations",base_allocation,'vs',other_allocation)
    ) + 
    theme_bw()
  
   ggplotly(p)
  # p
}


 diff_allocation(info,'1-1','1-3','sigma_xgrouptwo_length','Interval Length')
 

```

Going to wait until more sims come in

```{r}

diff_allocation(info,'1-1','1-5','sigma_xgrouptwo_length','Interval Length')

```

Going to wait until more sims come in

```{r}


grouptwo_info <- info %>%
  ggplot(aes(x = sample_size,
             y = sigma_xgrouptwo_cover,
             color = allocation,
             group = allocation,
             # ymin = xgrouptwo_q2.5,
             # ymax = xgrouptwo_q97.5,
             text = paste(
                          '</br>group one sample size: ',n1,
                          '</br>group two sample size: ',n2
             ))) +
  # geom_hline(yintercept = c(0, .5), color = "white") +
  # geom_pointrange(fatten = 1/2) +
  # scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) + 
  # geom_point() +
  geom_line() + 
  # ylim(min(info$xgrouptwo_q2.5) - 0.5,max(info$xgrouptwo_q97.5) + 0.5) + 
  # geom_hline(yintercept = 0.5) + 
  labs(x = "Sample Size (Total)",
       y = "Proportion",
       title = "Scale Interval coverage across Allocations") + 
  theme_bw() 


ggplotly(grouptwo_info) %>% 
  layout(hovermode = "x")
```

Finally we have a comparison of the credible interval length over the scale effect. That is how often was the 0.7 interval length satisfied for a particular sample size across allocations.

In conclusion for problem two, the unequal allocation serves to better estimate the parameters as the sample size increases. There is merit to allocation samples according to the ratio of variances between the groups with those that have a larger variance having more samples.

### Problem Three {#problem-three}

Simple Linear Regression

### Problem Four

Linear Regression with both continuous and categorical variables both being random

### 

## Conclusions

Similar conclusions as for the location effect sizes seem to hold for the scale effect size.

This raises a question as to whether post hoc power/sample sizes should be carried out. Some places advise against this but the initial power analysis is carried out under fictional circumstances imagined by the analyst to be the best representations of expected reality. How then are they to know that once the data is collected the same envisioned situations cover the dataset that has materialized. Care has to be taken with the differences due to variances that are present among groups; are power analysis usually complex enough to cover situations where differences in variances require more data to establish effect sizes.
