---
title: "Monte Carlo Power Analysis: Independent Two Sample Difference of Means"
description: |
  A short description of the post.
author:
  - name: Gavin Gordon
    url: https://example.com/norajones
date: 10-25-2021
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r message=FALSE,warning=F,echo=FALSE}
library(tidyverse)
library(gganimate)
library(plotly)
library(sjPlot)
library(brms)
library(truncdist)
library(pander)
library(gifski)
```

```{r}

file_path =  "C:/Users/gavin/Documents/blog_power_files/linear_regression/problem_three/"


```

```{r}

diff_allocation <- function(
  dataset,
  base_allocation,
  other_allocation,
  criteria,
  crit_name
  ){
  

to_diff <- dataset %>% 
  filter(allocation == base_allocation) %>% 
  dplyr::select(
    criteria,
    sample_size
  )


names(to_diff) <- c('base_allocate','sample_size')



for(i in 1:length(other_allocation)){
  
  varname <- paste('diff_',i,sep = "")
  allocate_name <- paste('allocate_',i,sep = "")  
  
to_diff[[allocate_name]] <- dataset %>% 
  filter(allocation == other_allocation[i]) %>% 
  dplyr::pull(criteria)


to_diff[[varname]] <- to_diff[['base_allocate']] - to_diff[[allocate_name]]

}


to_diff <- to_diff %>% 
  dplyr::select(!c(starts_with('allocate'),base_allocate)) %>% 
  pivot_longer(!sample_size,
               names_to = 'Allocation') %>%
  mutate(Allocation = factor(Allocation,labels = other_allocation)) 
  

to_diff$n1 <- 0
to_diff$n2 <- 0

for(i in 1:nrow(to_diff)){
  allocate_adjust <- as.numeric(unlist(str_split(to_diff$Allocation[i],"-")))
  number_batches = floor(to_diff$sample_size[i]/(allocate_adjust[1] + allocate_adjust[2]))
  n1 = number_batches * allocate_adjust[1]
  n2 = number_batches * allocate_adjust[2]
  to_diff$n1[i] = n1
  to_diff$n2[i] = n2
  to_diff$sample_size[i] = n1 + n2
}

p <- to_diff %>% 
ggplot(
    aes(
      sample_size,
      value,
      group = Allocation,
      color = Allocation,
      text = paste(
        '</br>group one sample size: ',n1,
        '</br>group two sample size: ',n2
      ))
  ) + 
  geom_line() + 
  # labs(
  #   x = "Sample Size",
  #   y = "Difference",
  #   title = paste('Difference between',crit_name,"for allocations",base_allocation,'vs',other_allocation[1])
  # ) + 
  theme_bw() + 
  # scale_color_discrete(labels = other_allocation) + 
  geom_hline(yintercept = 0) + 
  labs(
    color = 'Allocations',
    title = paste0('Drop in ',crit_name, ' relative to 1-1 Allocation'),
    x = 'Sample Size',
    y = 'Magnitude'
  ) + 
  theme(plot.title = element_text(hjust = 0.5))
  
ggplotly(p)
  # p
}

```

# Motivation

I found it discouraging that a power analysis for a multinomial multiple logistic regression wasn't readily accessible/available in the same vein as one could use the technique. Eventually I started to grasp why it might be difficult to distill the complexity of a dataset to a few much less one formula for a sample size while also having a certain flexibility to vary the conditions of the expected data.

This led me to favor simulations and such is first a proof of exercise, hopefully one that will generalize to any/most forms of modeling exercises one could undertake for determining a sample size as well as add to my capacity for performing simulations. As modeling becomes more complex such as with hierarchical models, simulations seem to become inevitable so this is also me setting the ground work.

A Sample Size analysis forces you to become intimate with your expectations from data, to consider many ways in which your data might materialize, while also serving as management tool for providing information for cost analysis and possible consideration for interim analysis (such as for clinical trials which I hope to make a general use case of). Simulations extend this capability by allowing you to include many additional variables while being able to vary the relationships you between these variables. You would not have to depend on sourcing prepackaged code for sample size calculations when considering different analysis as you are in full control of the data generating mechanism; you only need what is likely already available to you in terms of the models you expect to run.... and also hardware specs that won't deplete your patience.

## Intuition behind the technique

Basically you create datasets, and perform the analysis you expect to do. This data generation process is guided by the knowledge sourced from your literature review so it is imperative you examine the results in detail to understand what kinds of datasets you may encounter.

You simulate thousands of datasets and check how well a particular sample size will capture an effect you're either interested in or one you think exists.

<center>

```{r echo=F,eval=T}

build_sample <- function(
  nsims,
  u1,
  effect_size_u,
  allocate_n1,
  allocate_n2,
  sample_size,
  sigma_u1,
  effect_size_sigma
                         ){
  
  # get the sample size right for the two groups while respecting the allocations 
  number_batches = floor(sample_size/(allocate_n1 + allocate_n2))
  n1 = number_batches * allocate_n1
  n2 = number_batches * allocate_n2
  total_sample_size = n1 + n2
  
  
  # create the coefficient matrix
  treatment_vector = c(rep(0,n1),
                       rep(1,n2))
  
  intercept_vector = rep(1,total_sample_size)
  
  x_matrix <-   matrix(c(intercept_vector,
                         treatment_vector),
                       nrow = total_sample_size)
  
  
  # create the mean regression vector
  coeff_mean_vector <- matrix(c(u1,effect_size_u))
  u_regression <- x_matrix %*% coeff_mean_vector
  
  
  # create the variance regression vector
  coeff_sigma_vector <- matrix(c(sigma_u1,effect_size_sigma))
  sigma_regression <- x_matrix %*% coeff_sigma_vector
  
  
  y <- rnorm(total_sample_size,
             mean = u_regression,
             sd = sigma_regression)
  
  x <- c(rep("group one",n1),rep("group two",n2))
  
  
  df <- data.frame(x = factor(x),
                   y = y)
  # hist(c(y1,y2))
  
  
  return(df)
}


dataframes <- lapply(1:50,
                     build_sample,
                     u1 = 2,
                     effect_size_u = 3,
                     allocate_n1 = 1,
                     allocate_n2 = 1,
                     sample_size = 50,
                     sigma_u1 = 1,
                     effect_size_sigma = 0
                     )

dataframes <- bind_rows(dataframes, .id = "column_label")

p <- ggplot(dataframes, aes(fill = factor(x), y)) + 
  geom_histogram() + 
  geom_vline(xintercept = 2,
             color = 'red') + 
  geom_vline(xintercept = 5,
             color = 'blue') + 
  theme_bw() + 
  labs(
    title = "Example Simulations with Location Effect Size of Three",
    x = "Y values",
    y = "Count",
    fill = "Groups"
  ) + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  # Here comes the gganimate code
  transition_states(
    column_label,
    transition_length = 7,
    state_length = 5
  ) +
  enter_fade() + 
  exit_shrink() +
  ease_aes('sine-in-out')

animate(p,fps = 1,renderer = gifski_renderer())
```

</center>

The above animation should give an easy enough view of data generation. The data is from two Normal Distributions that have the same variance but their mode of central tendency are typically three units apart. After being able to simulate datasets of interest, now comes the application of whatever analysis you have in mind. For our purposes a regression approach is used (brms package in R) utilizing the defaults for Bayesian estimation.

For these problems I make use of the ability to explicitly model the variances instead of using a pooled estimate of some kind (heterogeneous regression modeling). Even though I know that the variances are the same for the above animations, this will set the trend for the upcoming problem for dealing with unequal variances.

An example of the model output is as such:

<center>

```{r echo = F}

regress <-  readRDS("C:/Users/gavin/Documents/GitHub/myBlog/regression_sample_p1.rds")

tab_model(regress)
```

</center>

We have two intercepts and two effect sizes. This is because there is one regression equation that models the mean of the normal distribution given as $intercept + xgrouptwo*variable_{group}$ , and similarly there is another regression that models the variance of the normal distribution given as $sigma\_intercept + sigma\_xgrouptwo * variable_{group}$ . If we had assumed a constant variance for the two groups there would just be a single value for the variance equation.

The variances are given on a log scale and must be exponentiated before we can interpret on the same scale as our data as shown below:

<center>

```{r echo=F}

hyp <- c("exp(sigma_Intercept) = 0",
         "exp(sigma_Intercept + sigma_xgrouptwo) = 0")
pander(hypothesis(regress, hyp)$hypothesis[,2:5])
```

</center>

The relevant information is extracted (mostly the quantiles of the posterior distribution of parameters), checked for whatever criteria we may be interested in and then the entire process is repeated several times for any one sample size; and also in our case for different group allocation schemes.

The following have been useful considerations:

1.  Have we managed to recover the parameters used to generate the sample? If so how often have we done so; this is the equivalent of checking for power.
2.  How does the length of the credible interval change across the sample sizes? What length is suitable to my needs? This is considering the stability of the estimates and is something I would discuss with a client by framing the question as how much precision around a particular estimate would there have to be before the estimate becomes action worthy?
3.  Is worth considering an unequal treatment/sampling allocation (something other than a 1:1 ratio for our two category group variable)?
4.  Is the credible interval typically positioned away from values which I would consider there to be no effect? For instance having a sample size that would keep the credible interval of the location effect size a certain distance away from zero (should the effect exist).
5.  Is there anything of interest that can be said about the simulations that fail to satisfy a particular criteria or are these happening randomly?

For direction and code related to the problem of detecting a group location effect size for some of the above considerations I direct you to a series of posts by <a href = "https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-i/"> Solomon Kurz </a> .

## On to the Simulations

We will be exploring the linear regression for which we will have a grouping variable with different variances across the groups and also an additional parameter that introduces linearity to the dataset.

To start things off the below is an animation giving a rough idea of what kinds of data sets we are talking about.

<center>

```{r}


build_sample <- function(
  sample_specification
){
  
  
 
  # specify the conditions for data generation 
  u_treatment = rtrunc(1,'norm',mean = 2,sd = 0.2,a = 1, b = 3) #
  effect_size_u = rtrunc(1,'norm',mean = 1,sd = 0.2,a = 0.5, b = 1.5)
  allocate_n1 = sample(c(1),size = 1,replace = T)
  allocate_n2 = sample(c(1,3),size = 1,replace = T)
  sample_size = sample(seq(30,300),size = 1,replace = T)
  sigma_u = rtrunc(1,'gamma',shape = 1,scale = 1,a = 1, b = 1.5)
  effect_size_sigma = rtrunc(1,'gamma',shape = 1,scale = 1,a = 0.5, b = 1.5)
  continuous_effect_size = rtrunc(1,'norm',mean = 0.6,sd = 0.2,a = 0.3, b = 0.9)
  
  

  
  # get the sample size right for the two groups while respecting the allocations 
  number_batches = floor(sample_size/(allocate_n1 + allocate_n2))
  n1 = number_batches * allocate_n1
  n2 = number_batches * allocate_n2
  total_sample_size = n1 + n2

  
  
  # create the coefficient matrix
  treatment_vector = c(
    rep(0,n1),
    rep(1,n2)
    )
  
  continuous_vector = runif(
    total_sample_size,
    10,
    50
    )
  

  intercept_vector = rep(1,total_sample_size)
  
  x_matrix <-   matrix(c(intercept_vector,
                         treatment_vector,
                         continuous_vector),
                       nrow = total_sample_size)
  
  
  
  # create the mean regression vector
  coeff_mean_vector <- matrix(c(u_treatment,effect_size_u,continuous_effect_size))
  
  u_regression <- x_matrix %*% coeff_mean_vector
  
  
  
  # create the variance regression vector
  sigma_matrix <-   matrix(
    c(
      intercept_vector,
      treatment_vector
      ),
    nrow = total_sample_size
    )
  

  coeff_sigma_vector <- matrix(c(sigma_u,effect_size_sigma))
  
  sigma_regression <- sigma_matrix %*% coeff_sigma_vector
  
  
  # generate the dependent variable
  y <- rnorm(total_sample_size,
             mean = u_regression,
             sd = sigma_regression)
  
  

  
  treatment_vector <- c(rep("group one",n1),rep("group two",n2))
  
  
  df <- data.frame(x = continuous_vector,
                   treatment = factor(treatment_vector), 
                   y = y)


return(df)
}





# set up the specs for the data generation 

parameters_grid <- expand.grid(
  nsim = 1:50
)





#  create the list to feed into the future
list_to_sim <- parameters_grid %>%
  group_split(nsim) %>%
  map(unlist)



dataframes <- lapply( 
  X = list_to_sim,
  FUN  =  build_sample
)




dataframes <- bind_rows(dataframes, .id = "column_label")

p <- ggplot(dataframes, aes(x, y,color = treatment)) + 
  geom_point(size = 4 ) + 
  # geom_vline(xintercept = 2,
  #            color = 'red') + 
  # geom_vline(xintercept = 5,
  #            color = 'blue') + 
  theme_bw() + 
  labs(
    title = "Linearly Related variables with unequal Group Variances",
    x = "X",
    y = "Y",
    color = "Groups"
  ) + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  # Here comes the gganimate code
  transition_states(
    column_label,
    transition_length = 1
  ) +
  enter_fade() + 
  exit_shrink() +
  ease_aes('sine-in-out')


animate(p, fps=1,renderer = gifski_renderer())

```

</center>

We have two groups; the red points you will notice form a thinner band compared to the blue points which are likely to be more spread out. This is the consequence of the unequal variances of the two groups. We are basically saying that the linear relationship exits within both groups but it applies to the members of group one more than those of group two.

We have two different location effects at work here. One due to the binary variable represented by the blue and red points while the other is a continuous variable represented by the X axis. This situation where is one scale effect where the variability differs across the binary variable only; however you can imagine that we could have also considered the variability changing across the continuous variable that would result familiar patterns such as the funnel/cone shape when the equal variance assumption is not satisfied across the continuous variable.

For those interested in a model:

$$
Y_i \sim N(\theta_i,\sigma_i) \ \  \ \ \ for \ i \ from \ 1 \ to \ n
$$

$$
\theta_i =  \beta_1 + \beta_2*Variable_{group} + \beta_3*Variable_{continuous}  
$$

$$
\sigma_i = \beta_4 + \beta_5*Variable_{group}  
$$

Now comes the variation that will be introduced across the location and scale parameters. Below are the graphs that give the range of the estimates used to create the datasets. We need a range for the $\beta_2,\beta_3$ (location effects) and $\beta_5$ (scale effect) as these are the effect sizes that will determine the respective linear equations for the mean and variance of the population distribution. For each simulation one value will be randomly chosen from each of the below distributions. By simulating thousands of cases, we will explore the entire range of the joint parameter space of possibilities. Again this information will be sourced from a combination of your literature review and your own subject matter knowledge about what may be happening.

```{r fig.align='center',fig.cap= 'Distributions for Beta Effect Sizes'}
readRDS(
  paste0(file_path,"parameter_explore.rds")
)

```

For the continuous effect and group location effects we simulate both from normal distributions that have means 0.6 and 0.5 respectively and variances 0.2 and 0.1 respectively. The values for the difference in variance across the groups were simulated from a gamma distribution with mean and variance of 1. All of the distributions were truncated as shown in the graph.

## Criteria Checking

### Initial Range

The purpose of the line graphs shown below is to detail the information for a range of sample sizes for further inspection. This initial look is not to draw any hard conclusions but instead provide the insight as to what sample sizes we should be focusing in on.

The ability to detect the location effect for the grouping variable is first shown.

```{r}

readRDS(
   paste0(file_path,"grouptwo_location_detect_all_info")
)


```

For almost all samples we have a detection probability that is above 0.9 so as described before, we do not have to worry much as to whether we will recover the population parameter within the credible intervals. We notice that the 1-2 allocation maintains an almost steady detection probability of at least 0.95.

However when we consider the precision, the noise around the estimate may give us quite large credible intervals thus we may want to collect a larger sample to gain a certain level of precision. The next graph shows us how the interval length of the location effect changes across the different sample sizes.

```{r}
readRDS(  
    paste0(file_path,"grouptwo_location_all_info.rds")
)

```

The problem with the smaller sample sizes is that those tend to produce large interval lengths that may allocate a considerable amount of probability to regions which we know is quite far from the true effect size as we planned it. An interval length of 0.7 will be the criteria for this exercise. You may of course require much greater precision for your own purposes but as you can see this will require quite large sample sizes. The good news is for this problem the 1-2 allocation is just as good (better if zoomed in although the difference may be negligible) as the 1-1 allocation at achieving this level of precision. This may help with costs as it may be cheaper to sample from the group which requires the two members while only needing the one member from the more expensive group. Again that kind of detail is dependent on your situation.

For this problem we will consider the 1-2 allocation; It would seem that we should investigate the sample sizes of at least 350 to achieve our goal.

The next two graphs describe what happens with the scale effect size for the group variable. This effect size is responsible for the difference in variability noted earlier in the animation where the blue points were varying about its regression line more than the red points. Similar considerations as to the detection and interval length are considered for this problem.

<center>

```{r}

readRDS(
    paste0(file_path,"grouptwo_scale_detect_all_info")
)



```

```{r}

readRDS(
   paste0(file_path,"grouptwo_scale_all_info")
)

```

</center>

It should be noted that it seems to require a much smaller sample to pick up the scale effect size. This is because it is actually quite larger than the location effect size. The mean of the gamma distribution it is simulated from is one, thus is twice as large. It makes sense that a larger effect requires a much smaller sample size to detect.

The sample size requirement has not increased. A smaller sample size of about 190 may capture the group scale effect but it would not capture the group location effect. Thus we still maintain the sample needed as before.

Finally for the initial set we consider the continuous variable location effect.

<center>

```{r}

readRDS(
    paste0(file_path,"grouptwo_X_location_detect_all_info")
)

```

```{r}
readRDS(
    paste0(file_path,"grouptwo_X_location_all_info")
)


```

</center>

What is interesting about continuous effect sizes, is that even though the size isn't that much larger than the group effect size, small sample sizes are able to get really precise intervals on the estimate. I will cheat here by referring to the formula for the estimator for a beta coefficient. The variability of the betas are directly affected by how much variation is covered in the predictors; the larger the variation the better the estimates. This is basically saying that if you are able to collect information from a broad range of your continuous variable then this means you have more information contained in your predictor variable and this is reflected in the calculation of your continuous effect size.

In the simulations we have assumed a random sample was taken over some arbitrary range. You should take another look at the animation and notice that we always seems to have values spread out across a consistent range.

### Narrowed Range

The current sample size range will easily satisfy the detection and they have a good mean interval length. At this point the remainder of the criterias are computed and are available by hovering over the interval range for any sample size.

```{r}
readRDS(
  paste0(file_path,"samples_set_treatment_location")
)

```

Now here comes another level of protection you can offer yourself when it comes to estimate stability. With a particular sample size, you can have an average length across all the simulations that satisfy our needs. But just how often is this length criteria satisfied. Sample Sizes that are around our minimum initial range satisfies this almost 50\\% of the time.

This leads to another criteria similar to power, which is to say how often do we want an interval length of 0.7 to be satisfied. Before you say rush to say always really should have the resources to needed to guarentee this. Which again brings us back to your ability to perform some sort of risk analysis and understand what resources you are willing to expend to get the results your want to your satisfactory level of certainty. In other words, how sure do you want to be about these estimates before you are willing to act on the information.

If you want the interval length to be satisfied lets say 80\\% of the time then that takes our sample size to roughly 438.

And yet there is another level of validity to consider. We know that on average our effect size should be be about 0.5 so we may insist that our credible intervals be above a certain number lets say 0.1 roughly speaking. In order to get that level of precision 70\\% of the time that would take our sample up to 468.

For the rest of our exercise we will restrict the criteria to the interval length being less than or equal to 0.7 for roughly 80\\% of the simulations.

The following two graphs give the criteria for the group scale effect and the continuous location effect, all of which are already satisfied by the minimum sample size chosen for the group location effect.

```{r}
readRDS(
  paste0(file_path,"samples_set_sigma_treatment_scale")
)

```

```{r}
readRDS(
  paste0(file_path,"samples_set_x_location")
)

```

### Where does the criteria breakdown occur

Now that we have a sample size estimate that works for our needs. We now want to understand just what are the situations that occur that this sample size would not be able to handle.

The first graph below shows what happens across combinations of the group locations for groups one and two.

<center>

```{r}

readRDS(
   paste0(file_path,"breakdown_location.rds")
)

```

</center>

We have the baseline mean which is the mean for group one on the x axis, while on the y axis we have the effect size that we are trying to detect. It seems that for the roughly 20\\% of cases that this sample size fails for, it does not have anything to do with the magnitudes of the effect sizes as it seems to fail randomly.

The following gives a different picture where the effect sizes for the variances are concerned.

<center>

```{r}

readRDS(
   paste0(file_path,"breakdown_scale.rds")
)
```

</center>

And here we have a base level understanding of what is important in your experiment, and is especially important information for you to retrieve from your literature review.

There is a pattern to how the simulations fail with respect to how much variation there is in the simulation. There also seems to be an interaction type relationship where the failure rate of goes up depending on the levels of the sigma magnitude. This is to say that if you were to select a magnitude for the baseline sigma, the failure rate would depend on the corresponding magnitude of the effect size.

It seems that variation that exceeds a sum total of roughly 2.2 is where the simulations start to fail. So for this experiment, as long as the total variation does not exceed this amount you have a good assurance that the sample size will perform as expected

## Conclusions

Similar conclusions as for the location effect sizes seem to hold for the scale effect size.

This raises a question as to whether post hoc power/sample sizes should be carried out. Some places advise against this but the initial power analysis is carried out under fictional circumstances imagined by the analyst to be the best representations of expected reality. How then are they to know that once the data is collected the same envisioned situations cover the dataset that has materialized. Care has to be taken with the differences due to variances that are present among groups; are power analysis usually complex enough to cover situations where differences in variances require more data to establish effect sizes.
