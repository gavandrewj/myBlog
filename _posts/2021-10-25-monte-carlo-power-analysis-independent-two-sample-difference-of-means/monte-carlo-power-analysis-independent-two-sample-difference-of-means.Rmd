---
title: "Monte Carlo Power Analysis: Independent Two Sample Difference of Means"
description: |
  A short description of the post.
author:
  - name: Gavin Gordon
    url: https://example.com/norajones
date: 10-25-2021
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r message=FALSE,warning=F,echo=FALSE}
library(tidyverse)
library(gganimate)
library(plotly)
library(sjPlot)
library(brms)
library(truncdist)
library(pander)
library(gifski)
```

```{r}

file_path =  "C:/Users/gavin/Documents/GitHub/myBlog/documents/two_sample_power_files/problem_three/"

```

```{r}

diff_allocation <- function(
  dataset,
  base_allocation,
  other_allocation,
  criteria,
  crit_name
  ){
  

to_diff <- dataset %>% 
  filter(allocation == base_allocation) %>% 
  dplyr::select(
    criteria,
    sample_size
  )


names(to_diff) <- c('base_allocate','sample_size')



for(i in 1:length(other_allocation)){
  
  varname <- paste('diff_',i,sep = "")
  allocate_name <- paste('allocate_',i,sep = "")  
  
to_diff[[allocate_name]] <- dataset %>% 
  filter(allocation == other_allocation[i]) %>% 
  dplyr::pull(criteria)


to_diff[[varname]] <- to_diff[['base_allocate']] - to_diff[[allocate_name]]

}


to_diff <- to_diff %>% 
  dplyr::select(!c(starts_with('allocate'),base_allocate)) %>% 
  pivot_longer(!sample_size,
               names_to = 'Allocation') %>%
  mutate(Allocation = factor(Allocation,labels = other_allocation)) 
  

to_diff$n1 <- 0
to_diff$n2 <- 0

for(i in 1:nrow(to_diff)){
  allocate_adjust <- as.numeric(unlist(str_split(to_diff$Allocation[i],"-")))
  number_batches = floor(to_diff$sample_size[i]/(allocate_adjust[1] + allocate_adjust[2]))
  n1 = number_batches * allocate_adjust[1]
  n2 = number_batches * allocate_adjust[2]
  to_diff$n1[i] = n1
  to_diff$n2[i] = n2
  to_diff$sample_size[i] = n1 + n2
}

p <- to_diff %>% 
ggplot(
    aes(
      sample_size,
      value,
      group = Allocation,
      color = Allocation,
      text = paste(
        '</br>group one sample size: ',n1,
        '</br>group two sample size: ',n2
      ))
  ) + 
  geom_line() + 
  # labs(
  #   x = "Sample Size",
  #   y = "Difference",
  #   title = paste('Difference between',crit_name,"for allocations",base_allocation,'vs',other_allocation[1])
  # ) + 
  theme_bw() + 
  # scale_color_discrete(labels = other_allocation) + 
  geom_hline(yintercept = 0) + 
  labs(
    color = 'Allocations',
    title = paste0('Drop in ',crit_name, ' relative to 1-1 Allocation'),
    x = 'Sample Size',
    y = 'Magnitude'
  ) + 
  theme(plot.title = element_text(hjust = 0.5))
  
ggplotly(p)
  # p
}

```

# Motivation

I found it discouraging that a power analysis for a multinomial multiple logistic regression wasn't readily accessible/available in the same vein as one could use the technique. Eventually I started to grasp why it might be difficult to distill the complexity of a dataset to a few much less one formula for a sample size.

This led me to favor simulations. Okay everything in stats is a simulation but that itself wasn't evident to me for quite some time until I started diving into the Bayesian methods with respect to posterior predictive checks.

This is first a proof of exercise, hopefully one that will generalize to any form of modeling exercise one could undertake for determining sample sizes/power as well as add to my capacity for performing simulations. As modeling becomes more complex such as with hierarchical models, simulations become inevitable so this is also me setting the ground work.

A Power/Sample Size analysis forces you to become intimate with your expectations from data, to consider many possible analyses, while also serving as management tool for providing information for cost analysis and interim analysis (such as for clinical trials which I hope to make a general use case of). Simulations extend this capability by allowing you to include many additional variables while being able to vary the relationships you between these variables. You would not have to depend on sourcing prepackaged code for power/sample size calculations when considering different analysis as you are in full control of the data generating mechanism; you only need what may already be available to you in terms of the models you expect to run.... and also hardware specs that won't deplete your patience.

## Intuition behind the technique

Basically you create datasets, and perform the analysis you expect to do. This data generation process is guided by the knowledge sourced from your literature review so it is imperative you examine the results in detail to understand what kinds of datasets you may encounter.

You simulate thousands of datasets and check how well a particular sample size will capture an effect you're either interested in or one you think exists.

```{r echo=F,eval=T}

build_sample <- function(
  nsims,
  u1,
  effect_size_u,
  allocate_n1,
  allocate_n2,
  sample_size,
  sigma_u1,
  effect_size_sigma
                         ){
  
  # get the sample size right for the two groups while respecting the allocations 
  number_batches = floor(sample_size/(allocate_n1 + allocate_n2))
  n1 = number_batches * allocate_n1
  n2 = number_batches * allocate_n2
  total_sample_size = n1 + n2
  
  
  # create the coefficient matrix
  treatment_vector = c(rep(0,n1),
                       rep(1,n2))
  
  intercept_vector = rep(1,total_sample_size)
  
  x_matrix <-   matrix(c(intercept_vector,
                         treatment_vector),
                       nrow = total_sample_size)
  
  
  # create the mean regression vector
  coeff_mean_vector <- matrix(c(u1,effect_size_u))
  u_regression <- x_matrix %*% coeff_mean_vector
  
  
  # create the variance regression vector
  coeff_sigma_vector <- matrix(c(sigma_u1,effect_size_sigma))
  sigma_regression <- x_matrix %*% coeff_sigma_vector
  
  
  y <- rnorm(total_sample_size,
             mean = u_regression,
             sd = sigma_regression)
  
  x <- c(rep("group one",n1),rep("group two",n2))
  
  
  df <- data.frame(x = factor(x),
                   y = y)
  # hist(c(y1,y2))
  
  
  return(df)
}


dataframes <- lapply(1:50,
                     build_sample,
                     u1 = 2,
                     effect_size_u = 3,
                     allocate_n1 = 1,
                     allocate_n2 = 1,
                     sample_size = 50,
                     sigma_u1 = 1,
                     effect_size_sigma = 0
                     )

dataframes <- bind_rows(dataframes, .id = "column_label")

p <- ggplot(dataframes, aes(fill = factor(x), y)) + 
  geom_histogram() + 
  geom_vline(xintercept = 2,
             color = 'red') + 
  geom_vline(xintercept = 5,
             color = 'blue') + 
  theme_bw() + 
  labs(
    title = "Example Simulations with Location Effect Size of Three",
    x = "Y values",
    y = "Count",
    fill = "Groups"
  ) + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  # Here comes the gganimate code
  transition_states(
    column_label,
    transition_length = 7,
    state_length = 5
  ) +
  enter_fade() + 
  exit_shrink() +
  ease_aes('sine-in-out')

animate(p,fps = 1,renderer = gifski_renderer())
```

The above animation should give an easy enough view of data generation. The data is that from two Normal Distributions that have the same variance but their mode of central tendency are typically three units apart. Now that we are able to simulate datasets, now comes the application of whatever analysis you have in mind. For our purposes a regression approach is used (brms package in R) utilizing the defaults for Bayesian estimation.

For these problems I make use of the ability to explicitly model the variances instead of using a pooled estimate of some kind (heterogeneous regression modeling). Even though I know that the variances are the same for the above animations, this will set the trend for the upcoming problems for dealing with unequal variances.

An example of the model output is as such:

<center>

```{r echo = F}

regress <-  readRDS("C:/Users/gavin/Documents/GitHub/myBlog/regression_sample_p1.rds")

tab_model(regress)
```

</center>

We have two intercepts and two effect sizes. This is because there is one regression equation that models the mean of the normal distribution, and similarly there is another regression that models the variance of the normal distribution.

The variances are given on a log scale and must be exponentiated before we can interpret on the same scale as our data as shown below

<center>

```{r echo=F}

hyp <- c("exp(sigma_Intercept) = 0",
         "exp(sigma_Intercept + sigma_xgrouptwo) = 0")
pander(hypothesis(regress, hyp)$hypothesis[,2:5])
```

</center>

The relevant information is extracted (mostly the quantiles of the posterior distribution of parameters) and then the entire process is repeated several times for any one sample size and also in our case for different group allocation schemes.

I am interested in the following:

1.  Have we managed to recover the parameters used to generate the sample? Making sure you check for all the parameters becomes important such as in situations when you expect to have unequal variances between groups.
2.  Is the length of the credible interval small enough? This is something I would discuss with a client. How much precision around the estimate would there have to be before the estimate becomes action worthy?
3.  Is worth considering an unequal treatment/sampling allocation (something other than a 1:1 ratio)?
4.  Is the credible interval typically positioned away from values which I would consider there to be no effect?

## On to the Simulations

### Problem One

The first problem we will consider is to determine a sample size that can recover the location and scale effects given below with a credible interval length of 0.7 for the precision. The difference in means is 0.5 magnitude with the variance between the two groups being the same but over varying group allocations (1-1,1-3,1-5).

$$
\mu_1 \sim N(2,1) 
$$

$$
\mu_2 \sim N(2.5,1)
$$

$$
e \sim N(0,1)
$$

$$
sigma ~ \gamma()
$$

The first useful piece of information is that for the estimated difference in the means.

<center>

```{r echo = F,fig.width= 7}
info <- readRDS("C:/Users/gavin/Documents/GitHub/myBlog/documents/two_sample_power_files/all_problems.rds")  %>%
  filter(para_com == "2 - 0.5 - 1 - 0")

pop_u = 2
pop_effect_size_u = 0.5 
pop_sigma_u1 = 1
pop_effect_size_sigma = 0


grouptwo_info <- info %>%
  filter(para_com == "2 - 0.5 - 1 - 0") %>% 
  ggplot(aes(x = sample_size,
             y = xgrouptwo,
             color = allocation,
             ymin = xgrouptwo_q2.5,
             ymax = xgrouptwo_q97.5,
             text = paste('interval length: ', xgrouptwo_length,
                          '</br>interval length criteria satisfied: ', xgrouptwo_cover,
                          '</br>detect probability: ', xgrouptwo_detect,
                          '</br>group one sample size: ',n1,
                          '</br>group two sample size: ',n2
             ))) +
  geom_hline(yintercept = c(0, .5), color = "white") +
  geom_pointrange(fatten = 1/2) +
  # scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) + 
  geom_point() + 
  ylim(min(info$xgrouptwo_q2.5) - 0.5,max(info$xgrouptwo_q97.5) + 0.5) + 
  geom_hline(yintercept = 0.5) + 
  labs(x = "Sample Size (Total)",
       y = "Location Effect Size",
       title = "Properties of the Location Effect across Sample Sizes") + 
  theme_bw() 


ggplotly(grouptwo_info) %>% 
  layout(hovermode = "x")


```

</center>

By hovering over a particular point we have access to more information about that particular sample size.

Almost all samples shown have the ability to include the location effect size a minimum of `r min(info$xgrouptwo_detect)` proportion of the time across all allocations. It seems that you don't quite need to worry about detecting the effect though the uncertainty around the estimates can be so terrible as to make the estimate untrustworthy as can be seen by the wide credible intervals for the smaller sample sizes.

This is why we directly consider the length of the intervals. The criteria set for this simulation was that of 0.7 for the credible interval; the label of xgrouptwo_cover will show the proportion of times this criteria was satisfied. The 1-1 allocation first met this requirement for sample sizes `r info %>% filter(allocation == '1-1',xgrouptwo_length < 0.7) %>% dplyr::select(sample_size) %>% min()` but was not able to maintain this for 0.8 proportion of the times until the sample size was `r info %>% filter(allocation == '1-1',xgrouptwo_cover >= 0.8) %>% dplyr::select(sample_size) %>% min()` . For the 1-3 allocation these sample sizes were `r info %>% filter(allocation == '1-3',xgrouptwo_length < 0.7) %>% dplyr::select(sample_size) %>% min()` and `r info %>% filter(allocation == '1-3',xgrouptwo_cover >= 0.8) %>% dplyr::select(sample_size) %>% min()` respectively. For the 1-5 allocation the sample sizes were `r info %>% filter(allocation == '1-5',xgrouptwo_length < 0.7) %>% dplyr::select(sample_size) %>% min()` and `r info %>% filter(allocation == '1-5',xgrouptwo_cover >= 0.8) %>% dplyr::select(sample_size) %>% min()` respectively. Even though the length of the interval may be precise, what if it actually tends to occurs near values for which we dismiss the effect size as non-existent. This may not be of concern for larger effect sizes say those greater than 1, but as what you are trying to detect becomes smaller, the proximity to the zero value can become concerning. Of course bayesian estimation gives the probability of an interval around the zero value so this alleviates the concern to some extent. Even so this is a problem I've noticed with some power analysis in that it gives the minimum sample size needed to reject the Null Hypothesis, for which in this case it would be the sample size needed to exclude zero from the interval. The problem is that zero isn't exactly that far from the interval when using that sample size, thus you would have to be extremely precise in your prediction of the true variability if you were to take that sample size at face value.

Therefore one might also consider for what sample size does the lower bound of the credible interval tend to be above a constant for example 0.1; for the 1-1 allocation this first occurs at `r info %>% filter(allocation == '1-1', xgrouptwo_q2.5 > 0.1) %>% dplyr::select(sample_size) %>% min()` and it was not able to maintain this for 80% of the times until a sample size of `r info %>% filter(allocation == '1-1', xgrouptwo_q2.5_0.1above >= 0.8) %>% dplyr::select(sample_size) %>% min()` ;for 1-3 allocation this happened at sample sizes `r info %>% filter(allocation == '1-3', xgrouptwo_q2.5 > 0.1) %>% dplyr::select(sample_size) %>% min()` and `r info %>% filter(allocation == '1-3', xgrouptwo_q2.5_0.1above >= 0.8) %>% dplyr::select(sample_size) %>% min()` respectively; for 1-5 allocation at `r info %>% filter(allocation == '1-5', xgrouptwo_q2.5 > 0.1) %>% dplyr::select(sample_size) %>% min()` and `r info %>% filter(allocation == '1-5', xgrouptwo_q2.5_0.1above >= 0.8) %>% dplyr::select(sample_size) %>% min()` respectively.

The next informative parameter is that of the scale effect. It is important that this is captured well as this will decide whether your data is too noisy to even begin to interpret what you gathered. Remember that for this situation the variances are the same so we expect this scale effect parameter to be zero.

```{r echo=FALSE}

grouptwo_sd_info <- info %>%
  filter(para_com == "2 - 0.5 - 1 - 0") %>% 
  ggplot(aes(x = sample_size,
             y = sigma_xgrouptwo,
             color = allocation,
             ymin = sigma_xgrouptwo_q2.5,
             ymax = sigma_xgrouptwo_q97.5,
             text = paste('interval length: ', sigma_xgrouptwo_length,
                          '</br>interval length criteria satisfied: ', sigma_xgrouptwo_cover,
                          '</br>detect probability: ', sigma_xgrouptwo_detect,
                          '</br>group one sample size: ',n1,
                          '</br>group two sample size: ',n2
             ))) +
  geom_hline(yintercept = c(0, .5), color = "white") +
  geom_pointrange(fatten = 1/2) +
  # scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) + 
  geom_point() + 
  ylim(min(info$sigma_xgrouptwo_q2.5) - 0.5,max(info$sigma_xgrouptwo_q97.5) + 0.5) + 
  geom_hline(yintercept = 0) + 
  labs(x = "Sample Size (Total)",
       y = "Location Effect Size",
       title = "Properties of the Scale Effect across Sample Sizes") + 
  theme_bw() 


ggplotly(grouptwo_sd_info) %>% 
  layout(hovermode = "x")


```

All samples shown have the ability to include the scaleeffect size a minimum of `r min(info$sigma_xgrouptwo_detect)` proportion of the time across all allocations.

The length of the intervals. The criteria set for this simulation was that of 0.7 for the credible interval; the label of sigma_xgrouptwo_cover will show the proportion of times this criteria was satisfied. The 1-1,1-3 and 1-5 allocations met this requirement for sample sizes `r info %>% filter(allocation == '1-1',sigma_xgrouptwo_length < 0.7) %>% dplyr::select(sample_size) %>% min()` , `r info %>% filter(allocation == '1-3',sigma_xgrouptwo_length < 0.7) %>% dplyr::select(sample_size) %>% min()` , and `r info %>% filter(allocation == '1-5',sigma_xgrouptwo_length < 0.7) %>% dplyr::select(sample_size) %>% min()` respectively.

Since this estimate is centered around zero, I let the length of the interval serve as the limit for upper and lower bounds of the estimate.

Thus in summary the following table details for each allocation the sample size requirements if we want the following to be satisfied at least 80% of the time:

1.  location effect to be captured.

2.  location effect credible interval to have a length less than 0.7

3.  location effect credible interval lower bound to be above 0.1

4.  scale effect credible interval to have a length of 0.7

```{r}
allo_1_total_samplesize <- info %>% filter(allocation == '1-1',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(sample_size) %>% min()

allo_1_n1_samplesize <- info %>% filter(allocation == '1-1',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(n1) %>% min()

allo_1_n2_samplesize <- info %>% filter(allocation == '1-1',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(n2) %>% min()


# sample sizes for 1-3 allocation
allo_3_total_samplesize <- info %>% filter(allocation == '1-3',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(sample_size) %>% min()

allo_3_n1_samplesize <- info %>% filter(allocation == '1-3',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(n1) %>% min()

allo_3_n2_samplesize <- info %>% filter(allocation == '1-3',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(n2) %>% min()


# sample size for 1-5 allocation 
allo_5_total_samplesize <- info %>% filter(allocation == '1-5',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(sample_size) %>% min()

allo_5_n1_samplesize <- info %>% filter(allocation == '1-5',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(n1) %>% min()

allo_5_n2_samplesize <- info %>% filter(allocation == '1-5',xgrouptwo_detect >= 0.8,xgrouptwo_cover >=0.8,xgrouptwo_q2.5_0.1above >= 0.8 ,sigma_xgrouptwo_cover > 0.8) %>% dplyr::select(n2) %>% min()


sample_size_tab <- data.frame(
  allocation = c(
    "1-1",
    "1-3",
    "1-5"
  ),
  total = c(
    allo_1_total_samplesize,
    allo_3_total_samplesize,
    allo_5_total_samplesize
    ),
  n1 = c(
    allo_1_n1_samplesize,
    allo_3_n1_samplesize,
    allo_5_n1_samplesize
  ),
  n2 = c(
    allo_1_n2_samplesize,
    allo_3_n2_samplesize,
    allo_5_n2_samplesize
  )
)

names(sample_size_tab) <- c(
  "Allocation",
  "Total Sample Size",
  "Group One Sample Size",
  "Group Two Sample Size"
)


pander(sample_size_tab)

```

### Problem Two

Now we will consider a problem that deals with a larger location effect size but with the complication of unequal variances.

$$
\mu_1 \sim N(2,1) 
$$

$$
\mu_2 \sim N(2.5,2)
$$

$$
e \sim N(0,1)
$$

For this problem we will not be calculating the sample size requirements as you will see it would require simulations for total sample sizes that exceed 400. Instead we will compare how the gains over the allocations compare to each other across the different criteria.

The first criteria of selection will be the proportion of times we were able to detect the estimate. You may recall this went well for the first problem and there does not seem to be a poor performance in the plot below.

<center>

```{r echo = F,fig.width= 7}
info <- readRDS("C:/Users/gavin/Documents/GitHub/myBlog/documents/two_sample_power_files/all_problems.rds")  %>%
  filter(para_com == "2 - 0.5 - 1 - 2")



grouptwo_info <- info %>%
  ggplot(aes(x = sample_size,
             y = xgrouptwo_detect,
             color = allocation,
             group = allocation,
             # ymin = xgrouptwo_q2.5,
             # ymax = xgrouptwo_q97.5,
             text = paste(
                          '</br>group one sample size: ',n1,
                          '</br>group two sample size: ',n2
             ))) +
  # geom_hline(yintercept = c(0, .5), color = "white") +
  # geom_pointrange(fatten = 1/2) +
  # scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) + 
  # geom_point() +
  geom_line() + 
  # ylim(min(info$xgrouptwo_q2.5) - 0.5,max(info$xgrouptwo_q97.5) + 0.5) + 
  # geom_hline(yintercept = 0.5) + 
  labs(x = "Sample Size (Total)",
       y = "Proportion",
       title = "Properties of the Location Detect Proportions across Allocations") + 
  theme_bw() 


ggplotly(grouptwo_info) %>% 
  layout(hovermode = "x")


```

</center>

```{r}
 diff_allocation(info,'1-1',c('1-3','1-5'),'xgrouptwo_detect','Detection of Group Two Location Effect')

```

Not sure what to comment right now about this. Going to wait until I am able to run more simulations.

The next criteria is the length of the intervals. Immediately we can see that the 1-1 allocation is always performing worse for all sample sizes relative to the other allocations.

```{r echo=FALSE}



grouptwo_info <- info %>%
  ggplot(aes(x = sample_size,
             y = xgrouptwo_length,
             color = allocation,
             group = allocation,
             # ymin = xgrouptwo_q2.5,
             # ymax = xgrouptwo_q97.5,
             text = paste(
                          '</br>group one sample size: ',n1,
                          '</br>group two sample size: ',n2
             ))) +
  # geom_hline(yintercept = c(0, .5), color = "white") +
  # geom_pointrange(fatten = 1/2) +
  # scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) + 
  # geom_point() +
  geom_line() + 
  # ylim(min(info$xgrouptwo_q2.5) - 0.5,max(info$xgrouptwo_q97.5) + 0.5) + 
  # geom_hline(yintercept = 0.5) + 
  labs(x = "Sample Size (Total)",
       y = "Proportion",
       title = "Properties of the Location Effect Interval Length across Allocations") + 
  theme_bw() 


ggplotly(grouptwo_info) %>% 
  layout(hovermode = "x")


```

The 1-3 allocation seems to be performing slightly better than the 1-5 allocation across all sample sizes. This difference seems to decrease as the sample sizes increase. Overall it would be better to use the 1-3 allocation. Interestingly the variance for group two is three times that of group one and so the allocation that is based on the ratio of variability works best.

Now the question becomes, if we know that the allocations seem to perform better, just how much better? The following graphs **compares the drop in Interval length for the group two location effect size.**

```{r}

 diff_allocation(info,'1-1',c('1-3','1-5'),'xgrouptwo_length','Interval Length')

```

We can see that early on with the smaller sample sizes the gain is quite large with the 1-3 allocation benefiting with a 0.4 reduction in the interval length. This benefit decreases over time but seems to reach an almost constant level at around 0.1 for the larger sample sizes.

The smaller samples for the 1-5 allocations have difficult time compared to the 1-1 allocation but similarly become more beneficial as the sample size increases while also reaching some sort of gently decreasing boundary for the larger sample sizes.

Will wait for more sims to give more solid numbers.

```{r}



grouptwo_info <- info %>%
  ggplot(aes(x = sample_size,
             y = xgrouptwo_q2.5_0.1above,
             color = allocation,
             group = allocation,
             # ymin = xgrouptwo_q2.5,
             # ymax = xgrouptwo_q97.5,
             text = paste(
                          '</br>group one sample size: ',n1,
                          '</br>group two sample size: ',n2
             ))) +
  # geom_hline(yintercept = c(0, .5), color = "white") +
  # geom_pointrange(fatten = 1/2) +
  # scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) + 
  # geom_point() +
  geom_line() + 
  # ylim(min(info$xgrouptwo_q2.5) - 0.5,max(info$xgrouptwo_q97.5) + 0.5) + 
  # geom_hline(yintercept = 0.5) + 
  labs(x = "Sample Size (Total)",
       y = "Proportion",
       title = "Properties of the Location Detect Proportions across Allocations") + 
  theme_bw() 


ggplotly(grouptwo_info) %>% 
  layout(hovermode = "x")

```

Although I will wait for more simulations to be run, it does seem like the 1-3 allocation seems to be higher in the proportion than most sample sizes when compared to the 1-1 allocation. It is not clear that what the pattern is for the 1-3 compared to the 1-5 allocation as these seem to have similar performance.

```{r}
 diff_allocation(info,'1-1',c('1-3','1-5'),'xgrouptwo_q2.5_0.1above','Intervals above 0.1')
```

```{r}



grouptwo_info <- info %>%
  ggplot(aes(x = sample_size,
             y = sigma_xgrouptwo_length,
             color = allocation,
             group = allocation,
             # ymin = xgrouptwo_q2.5,
             # ymax = xgrouptwo_q97.5,
             text = paste(
                          '</br>group one sample size: ',n1,
                          '</br>group two sample size: ',n2
             ))) +
  # geom_hline(yintercept = c(0, .5), color = "white") +
  # geom_pointrange(fatten = 1/2) +
  # scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) + 
  # geom_point() +
  geom_line() + 
  # ylim(min(info$xgrouptwo_q2.5) - 0.5,max(info$xgrouptwo_q97.5) + 0.5) + 
  # geom_hline(yintercept = 0.5) + 
  labs(x = "Sample Size (Total)",
       y = "Proportion",
       title = "Properties of the Scale Effect Interval Length across Allocations") + 
  theme_bw() 


ggplotly(grouptwo_info) %>% 
  layout(hovermode = "x")
```

Here we see the same pattern as with the Location effect interval where the 1-1 allocation performed worse compared to the other allocations, and of the remainder the 1-3 allocation performed better.

The question we may have is whether the difference in interval length is enough to consider using one allocation to the other.

```{r}




  diff_allocation(info,'1-1',c('1-3','1-5'),'sigma_xgrouptwo_length','Interval Length of Group Two Scale Effect')

```

The above graph gives us an idea of how much more efficient the allocations get when compared to a baseline being the 1-1 allocation. When we are seeking to estimate small effect sizes a decrease in an interval length of even 0.1 would be hugely beneficial. The 1-3 allocation has the most gains consistent over the sample sizes while the 1-5 allocation seems to initially be worse than the 1-1 allocation but does become better although not as great as the 1-3.

Will wait on more sims

Going to wait until more sims come in

Going to wait until more sims come in

```{r}


grouptwo_info <- info %>%
  ggplot(aes(x = sample_size,
             y = sigma_xgrouptwo_cover,
             color = allocation,
             group = allocation,
             # ymin = xgrouptwo_q2.5,
             # ymax = xgrouptwo_q97.5,
             text = paste(
                          '</br>group one sample size: ',n1,
                          '</br>group two sample size: ',n2
             ))) +
  # geom_hline(yintercept = c(0, .5), color = "white") +
  # geom_pointrange(fatten = 1/2) +
  # scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) + 
  # geom_point() +
  geom_line() + 
  # ylim(min(info$xgrouptwo_q2.5) - 0.5,max(info$xgrouptwo_q97.5) + 0.5) + 
  # geom_hline(yintercept = 0.5) + 
  labs(x = "Sample Size (Total)",
       y = "Proportion",
       title = "Scale Interval coverage across Allocations") + 
  theme_bw() 


ggplotly(grouptwo_info) %>% 
  layout(hovermode = "x")
```

Finally we have a comparison of the credible interval length over the scale effect. That is how often was the 0.7 interval length satisfied for a particular sample size across allocations.

In conclusion for problem two, the unequal allocation serves to better estimate the parameters as the sample size increases. There is merit to allocation samples according to the ratio of variances between the groups with those that have a larger variance having more samples.

### Problem Three {#problem-three}

This time we will be exploring the linear regression for which we will have a grouping variable with different variances across the groups and also an additional parameter that introduces linearity to the dataset.

To start things off the below is an animation giving a rough idea of what kinds of data sets we are talking about.

<center>

```{r}


build_sample <- function(
  sample_specification
){
  
  
 
  # specify the conditions for data generation 
  u_treatment = rtrunc(1,'norm',mean = 2,sd = 0.2,a = 1, b = 3) #
  effect_size_u = rtrunc(1,'norm',mean = 1,sd = 0.2,a = 0.5, b = 1.5)
  allocate_n1 = sample(c(1),size = 1,replace = T)
  allocate_n2 = sample(c(1,3),size = 1,replace = T)
  sample_size = sample(seq(30,300),size = 1,replace = T)
  sigma_u = rtrunc(1,'gamma',shape = 1,scale = 1,a = 1, b = 1.5)
  effect_size_sigma = rtrunc(1,'gamma',shape = 1,scale = 1,a = 0.5, b = 1.5)
  continuous_effect_size = rtrunc(1,'norm',mean = 0.6,sd = 0.2,a = 0.3, b = 0.9)
  
  

  
  # get the sample size right for the two groups while respecting the allocations 
  number_batches = floor(sample_size/(allocate_n1 + allocate_n2))
  n1 = number_batches * allocate_n1
  n2 = number_batches * allocate_n2
  total_sample_size = n1 + n2

  
  
  # create the coefficient matrix
  treatment_vector = c(
    rep(0,n1),
    rep(1,n2)
    )
  
  continuous_vector = runif(
    total_sample_size,
    10,
    50
    )
  

  intercept_vector = rep(1,total_sample_size)
  
  x_matrix <-   matrix(c(intercept_vector,
                         treatment_vector,
                         continuous_vector),
                       nrow = total_sample_size)
  
  
  
  # create the mean regression vector
  coeff_mean_vector <- matrix(c(u_treatment,effect_size_u,continuous_effect_size))
  
  u_regression <- x_matrix %*% coeff_mean_vector
  
  
  
  # create the variance regression vector
  sigma_matrix <-   matrix(
    c(
      intercept_vector,
      treatment_vector
      ),
    nrow = total_sample_size
    )
  

  coeff_sigma_vector <- matrix(c(sigma_u,effect_size_sigma))
  
  sigma_regression <- sigma_matrix %*% coeff_sigma_vector
  
  
  # generate the dependent variable
  y <- rnorm(total_sample_size,
             mean = u_regression,
             sd = sigma_regression)
  
  

  
  treatment_vector <- c(rep("group one",n1),rep("group two",n2))
  
  
  df <- data.frame(x = continuous_vector,
                   treatment = factor(treatment_vector), 
                   y = y)


return(df)
}





# set up the specs for the data generation 

parameters_grid <- expand.grid(
  nsim = 1:50
)





#  create the list to feed into the future
list_to_sim <- parameters_grid %>%
  group_split(nsim) %>%
  map(unlist)



dataframes <- lapply( 
  X = list_to_sim,
  FUN  =  build_sample
)




dataframes <- bind_rows(dataframes, .id = "column_label")

p <- ggplot(dataframes, aes(x, y,color = treatment)) + 
  geom_point(size = 4 ) + 
  # geom_vline(xintercept = 2,
  #            color = 'red') + 
  # geom_vline(xintercept = 5,
  #            color = 'blue') + 
  theme_bw() + 
  labs(
    title = "Linearly Related variables with unequal Group Variances",
    x = "X",
    y = "Y",
    color = "Groups"
  ) + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  # Here comes the gganimate code
  transition_states(
    column_label,
    transition_length = 1
  ) +
  enter_fade() + 
  exit_shrink() +
  ease_aes('sine-in-out')


animate(p, fps=1,renderer = gifski_renderer())

```

</center>

We have two groups; the red points you will notice a thinner band compared to the blue points which are likely to be more spread out. This is the consequence of the unequal variances of the two groups. We are basically saying that the linear relationship exits within both groups but it applies to the members of group one more than those of group two.

We have two different location effects at work here. One due to the binary variable represented by the blue and red points while the other is a continuous variable represented by the X axis. As before we only consider a situation where the variability differs across the binary variable; you can imagine that we could have also considered the variance changing across the continuous variable that would result familiar patterns such as the funnel/cone shape when the equal variance assumption is not satisfied across the continuous variable.

Now comes the variation that will be introduced across the parameters. Below are the graphs that give the range of the estimates that we want to create samples for and to determine a sample that will recapture the generated effects for the majority of the time. Again this information will be sourced from a combination of your literature review and your own subject matter knowledge about what may be happening.

```{r fig.align='center'}
readRDS(
  paste0(file_path,"parameter_explore.rds")
)

```

For the continuous effect and group location effects we simulate both from normal distributions that have means 0.6 and 0.5 respectively and variances 0.2 and 0.1 respectively. The values for the difference in variance across the groups were simulated from a gamma distribution with scale and shape parameters of 1. All of the distributions were truncated as shown in the graph.

The first graph shown below details the information for location effect for the grouping variable. For almost all samples we have a detection probability that is above 0.9 so as described before, we do not have to worry much as to whether we will recover the population parameter within the credible intervals.

```{r}

readRDS(
   paste0(file_path,"grouptwo_location_detect_all_info")
)


```

However when we consider the precision, the noise around the estimate may give us quite large credible intervals thus we may want to collect a larger sample to gain a certain level of precision. For this problem we will seek a credible interval length of 0.7

```{r}
readRDS(  
    paste0(file_path,"grouptwo_location_all_info.rds")
)

```

```{r}

readRDS(
   paste0(file_path,"grouptwo_scale_all_info")
)

```

```{r}

readRDS(
    paste0(file_path,"grouptwo_scale_detect_all_info")
)

```

```{r}

readRDS(
    paste0(file_path,"grouptwo_X_location_all_info")
)
```

```{r}

readRDS(
    paste0(file_path,"grouptwo_X_location_detect_all_info")
)

```

```{r}
readRDS(
  paste0(file_path,"samples_set_treatment_location")
)

```

```{r}
readRDS(
  paste0(file_path,"samples_set_sigma_treatment_scale")
)

```

```{r}
readRDS(
  paste0(file_path,"samples_set_x_location")
)

```

Comment on the early, mid and late game behaviour of the allocations. It seems like the efficiency of the 1-1 is good early on but it eventually gets overtaken by the 1-2, then the 1-3 allocations. Nevmind they occupy the same regions with over-lapping confidence bands. So there are some benefits to knowing this, is that if there is a cost differential for getting items, where items from group two is cheaper to sample from, there is a point, once the sample becomes larger enough the 1-3 allocation would give you the same precision (slightly better) than having to maintain the 1-1 allocation driving costs up.

So next we will consider only sampling from the 1-2 allocation and add on some additional requirements that we would want the estimate to have.

The next informative parameter is that of the scale effect. It is important that this is captured well as this will decide whether your data is too noisy to even begin to interpret what you gathered. Remember that for this situation the variances are the same so we expect this scale effect parameter to be zero.

```{r}


```

```{r}



```

It seems that just about any sample size in the set we have considered manage to get a good precision as well

## Conclusions

Similar conclusions as for the location effect sizes seem to hold for the scale effect size.

This raises a question as to whether post hoc power/sample sizes should be carried out. Some places advise against this but the initial power analysis is carried out under fictional circumstances imagined by the analyst to be the best representations of expected reality. How then are they to know that once the data is collected the same envisioned situations cover the dataset that has materialized. Care has to be taken with the differences due to variances that are present among groups; are power analysis usually complex enough to cover situations where differences in variances require more data to establish effect sizes.
