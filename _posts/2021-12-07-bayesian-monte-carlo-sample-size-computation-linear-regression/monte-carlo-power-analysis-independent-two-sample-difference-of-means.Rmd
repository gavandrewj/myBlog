---
title: "Bayesian Monte Carlo Sample Size Computation: Linear Regression"
description: |
  This problem has one binary predictor variable for which the groups differ in variability (application of hetergenous modeling to capture variance difference instead of pooled methods) and one continuous variable. Focus on the precision of Credible Interval length which is subjected to various critera to satisfy the need for managing potential uncertainty.  
author:
  - name: Gavin Andrew Gordon
    url: https://example.com/norajones
date: 12-07-2021
output:
  distill::distill_article:
    self_contained: false
    
bibliography: biblio.bib
nocite: '@*'
---

```{r}
#TODO include citation to heterogenous modeling
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r message=FALSE,warning=F,echo=FALSE}
library(tidyverse)
library(gganimate)
library(plotly)
library(sjPlot)
library(brms)
library(truncdist)
library(pander)
library(gifski)
```

```{r}

file_path =  "C:/Users/gavin/Documents/blog_power_files/linear_regression/problem_three/"




#TODO move through and see what should get citations
#TODO reference bit as to why we choose precision over power

```

```{r}

diff_allocation <- function(
  dataset,
  base_allocation,
  other_allocation,
  criteria,
  crit_name
  ){
  

to_diff <- dataset %>% 
  filter(allocation == base_allocation) %>% 
  dplyr::select(
    criteria,
    sample_size
  )


names(to_diff) <- c('base_allocate','sample_size')



for(i in 1:length(other_allocation)){
  
  varname <- paste('diff_',i,sep = "")
  allocate_name <- paste('allocate_',i,sep = "")  
  
to_diff[[allocate_name]] <- dataset %>% 
  filter(allocation == other_allocation[i]) %>% 
  dplyr::pull(criteria)


to_diff[[varname]] <- to_diff[['base_allocate']] - to_diff[[allocate_name]]

}


to_diff <- to_diff %>% 
  dplyr::select(!c(starts_with('allocate'),base_allocate)) %>% 
  pivot_longer(!sample_size,
               names_to = 'Allocation') %>%
  mutate(Allocation = factor(Allocation,labels = other_allocation)) 
  

to_diff$n1 <- 0
to_diff$n2 <- 0

for(i in 1:nrow(to_diff)){
  allocate_adjust <- as.numeric(unlist(str_split(to_diff$Allocation[i],"-")))
  number_batches = floor(to_diff$sample_size[i]/(allocate_adjust[1] + allocate_adjust[2]))
  n1 = number_batches * allocate_adjust[1]
  n2 = number_batches * allocate_adjust[2]
  to_diff$n1[i] = n1
  to_diff$n2[i] = n2
  to_diff$sample_size[i] = n1 + n2
}

p <- to_diff %>% 
ggplot(
    aes(
      sample_size,
      value,
      group = Allocation,
      color = Allocation,
      text = paste(
        '</br>group one sample size: ',n1,
        '</br>group two sample size: ',n2
      ))
  ) + 
  geom_line() + 
  # labs(
  #   x = "Sample Size",
  #   y = "Difference",
  #   title = paste('Difference between',crit_name,"for allocations",base_allocation,'vs',other_allocation[1])
  # ) + 
  theme_bw() + 
  # scale_color_discrete(labels = other_allocation) + 
  geom_hline(yintercept = 0) + 
  labs(
    color = 'Allocations',
    title = paste0('Drop in ',crit_name, ' relative to 1-1 Allocation'),
    x = 'Sample Size',
    y = 'Magnitude'
  ) + 
  theme(plot.title = element_text(hjust = 0.5))
  
ggplotly(p)
  # p
}

```

# Motivation

I found it discouraging that a method for Sample Size Analysis for a multinomial multiple logistic regression wasn't readily accessible/available in the same vein as one could use the technique. Eventually I started to grasp why it might be difficult to distill the complexity of a dataset to a few formulas for a sample size.

And so I leaned into simulations. This post is first a proof of exercise; hopefully one that will generalize to any/most forms of modeling exercises one could undertake for determining a sample size as well as to add to my capacity for performing simulation studies.

A Sample Size Analysis forces you to become intimate with your expectations from data, to consider the many ways in which your data might materialize, while also serving as management tool for providing information to perform cost analysis with possible consideration for interim analysis (such as for clinical trials which I hope to make a general use case of). Simulations allow you to include and vary the relationships between many variables, the effects for which you want some assurance your sample size is able to capture if present.

There is no need to await prepackaged code for sample size calculations as you are in full control of the data generating mechanism and only need what is likely already available in terms of the models you expect to run.

## Intuition behind the technique

Basically you create datasets, and perform the analysis you expect to do. This data generation process is guided by the knowledge sourced from your literature review and your own subject matter knowledge, so it is imperative you examine the previous research in detail to understand what kinds of datasets you may encounter.

For each dataset you check how well a particular sample size will capture an effect you're interested in according to a set of criteria.

<center>

```{r echo=F,eval=T}

build_sample <- function(
  nsims,
  u1,
  effect_size_u,
  allocate_n1,
  allocate_n2,
  sample_size,
  sigma_u1,
  effect_size_sigma
                         ){
  
  # get the sample size right for the two groups while respecting the allocations 
  number_batches = floor(sample_size/(allocate_n1 + allocate_n2))
  n1 = number_batches * allocate_n1
  n2 = number_batches * allocate_n2
  total_sample_size = n1 + n2
  
  
  # create the coefficient matrix
  treatment_vector = c(rep(0,n1),
                       rep(1,n2))
  
  intercept_vector = rep(1,total_sample_size)
  
  x_matrix <-   matrix(c(intercept_vector,
                         treatment_vector),
                       nrow = total_sample_size)
  
  
  # create the mean regression vector
  coeff_mean_vector <- matrix(c(u1,effect_size_u))
  u_regression <- x_matrix %*% coeff_mean_vector
  
  
  # create the variance regression vector
  coeff_sigma_vector <- matrix(c(sigma_u1,effect_size_sigma))
  sigma_regression <- x_matrix %*% coeff_sigma_vector
  
  
  y <- rnorm(total_sample_size,
             mean = u_regression,
             sd = sigma_regression)
  
  x <- c(rep("group one",n1),rep("group two",n2))
  
  
  df <- data.frame(x = factor(x),
                   y = y)
  # hist(c(y1,y2))
  
  
  return(df)
}


dataframes <- lapply(1:50,
                     build_sample,
                     u1 = 2,
                     effect_size_u = 3,
                     allocate_n1 = 1,
                     allocate_n2 = 1,
                     sample_size = 200,
                     sigma_u1 = 1,
                     effect_size_sigma = 0
                     )

dataframes <- bind_rows(dataframes, .id = "column_label")

p <- ggplot(dataframes, aes(fill = factor(x), y)) + 
  geom_histogram() + 
  geom_vline(xintercept = 2,
             color = 'red') + 
  geom_vline(xintercept = 5,
             color = 'blue') + 
  theme_bw() + 
  labs(
    title = "Example Simulations with Location Effect Size of Three",
    x = "Y values",
    y = "Count",
    fill = "Groups"
  ) + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  # Here comes the gganimate code
  transition_states(
    column_label,
    transition_length = 7,
    state_length = 5
  ) +
  enter_fade() + 
  exit_shrink() +
  ease_aes('sine-in-out')

animate(p,fps = 1,renderer = gifski_renderer())
```

</center>

The above animation should give an easy enough understanding of data generation phase. The data is simulated from two Normal Distributions that have the same variance but their means are typically three units apart. After being able to simulate datasets of interest, now comes the application of whatever analysis you have in mind. For our purposes we're using a Bayesian regression approach using the R package BRMS @paul2018 .

For these problems I make use of the ability to explicitly model the variances instead of using a pooled estimate of some kind @paul2021 . Even though I know that the variances are the same for the above animation, this will set the trend for the upcoming problem for dealing with unequal variances.

An example of the model output is as such:

<center>

```{r echo = F}

regress <-  readRDS(
    paste0(file_path,'regression_model.rds')
)

tab_model(regress)
```

</center>

We have two intercepts and two effect sizes. This is because there is one regression equation that models the mean of the normal distribution and similarly there is another regression that models the variance of the normal distribution. If we had assumed a constant variance for the two groups there would just be a single value for the variance equation.

So for the two groups in the animation, their normal distributions are estimated as such:

$$
Y_{Group \ One} \sim N(intercept,sigma\_intercept)
$$

$$
Y_{Group \ Two} \sim N(intercept + xgrouptwo*variable_{group},sigma\_intercept + sigma\_xgrouptwo * variable_{group})
$$

Where $variable_{group}$ is an indicator variable for the second distribution. The variances are given on a log scale and must be exponentiated before we can interpret on the same scale as our data as shown below:

<center>

```{r echo=F}

hyp <- c("exp(sigma_Intercept) = 0",
         "exp(sigma_Intercept + sigma_xgrouptwo) = 0")
pander(hypothesis(regress, hyp)$hypothesis[,2:5])
```

</center>

The relevant information is extracted for each parameter which in this case is the interval determined by the range of parameters for which 95% (you can choose your own percentage) of the probability is allocated known as the Highest Density Probability Interval @wang2002simulation. This interval is then queried for whatever criteria we may be interested in.

The following have been useful considerations:

1.  Have we managed to recover the parameters used to generate the sample? If so how often have we done so?
2.  How does the length of the credible interval change across the sample sizes? What length is suitable to my needs? This is considering the stability of the estimates and is something I would discuss with a client by framing the question as how much precision around a particular estimate would there have to be before the estimate becomes action worthy?
3.  Is it worth considering an unequal sampling group allocation (something other than a 1:1 ratio for our two category group variable)?
4.  Is the credible interval typically positioned away from values which I would consider to be of no interest? For instance having a sample size that would keep the credible interval of the location effect size a certain distance away from zero (should the effect exist).
5.  Is there anything of interest that can be said about the simulations that fail to satisfy a particular criteria or are these happening randomly?

For further comment and code for detecting a group location effect size for the problem described in the first animation I direct you to a series of posts by <a href = "https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-i/"> @Kurz2019 </a> .

## On to the Simulations

We will be exploring the linear regression for which we will have a grouping variable with different variances across the groups, and a continuous variable that introduces linearity to the dataset.

To start things off the below is an animation giving a rough idea of what kinds of data sets we are talking about.

<center>

```{r eval=T}


build_sample <- function(
  sample_specification
){
  
  
 
  # specify the conditions for data generation 
  u_treatment = rtrunc(1,'norm',mean = 2,sd = 0.2,a = 1, b = 3) #
  effect_size_u = rtrunc(1,'norm',mean = 1,sd = 0.2,a = 0.5, b = 1.5)
  allocate_n1 = sample(c(1),size = 1,replace = T)
  allocate_n2 = sample(c(1,3),size = 1,replace = T)
  sample_size = sample(seq(30,300),size = 1,replace = T)
  sigma_u = rtrunc(1,'gamma',shape = 1,scale = 1,a = 1, b = 1.5)
  effect_size_sigma = rtrunc(1,'gamma',shape = 1,scale = 1,a = 0.5, b = 1.5)
  continuous_effect_size = rtrunc(1,'norm',mean = 0.6,sd = 0.2,a = 0.3, b = 0.9)
  
  

  
  # get the sample size right for the two groups while respecting the allocations 
  number_batches = floor(sample_size/(allocate_n1 + allocate_n2))
  n1 = number_batches * allocate_n1
  n2 = number_batches * allocate_n2
  total_sample_size = n1 + n2

  
  
  # create the coefficient matrix
  treatment_vector = c(
    rep(0,n1),
    rep(1,n2)
    )
  
  continuous_vector = runif(
    total_sample_size,
    10,
    50
    )
  

  intercept_vector = rep(1,total_sample_size)
  
  x_matrix <-   matrix(c(intercept_vector,
                         treatment_vector,
                         continuous_vector),
                       nrow = total_sample_size)
  
  
  
  # create the mean regression vector
  coeff_mean_vector <- matrix(c(u_treatment,effect_size_u,continuous_effect_size))
  
  u_regression <- x_matrix %*% coeff_mean_vector
  
  
  
  # create the variance regression vector
  sigma_matrix <-   matrix(
    c(
      intercept_vector,
      treatment_vector
      ),
    nrow = total_sample_size
    )
  

  coeff_sigma_vector <- matrix(c(sigma_u,effect_size_sigma))
  
  sigma_regression <- sigma_matrix %*% coeff_sigma_vector
  
  
  # generate the dependent variable
  y <- rnorm(total_sample_size,
             mean = u_regression,
             sd = sigma_regression)
  
  

  
  treatment_vector <- c(rep("group one",n1),rep("group two",n2))
  
  
  df <- data.frame(x = continuous_vector,
                   treatment = factor(treatment_vector), 
                   y = y)


return(df)
}





# set up the specs for the data generation 

parameters_grid <- expand.grid(
  nsim = 1:50
)





#  create the list to feed into the future
list_to_sim <- parameters_grid %>%
  group_split(nsim) %>%
  map(unlist)



dataframes <- lapply( 
  X = list_to_sim,
  FUN  =  build_sample
)




dataframes <- bind_rows(dataframes, .id = "column_label")

p <- ggplot(dataframes, aes(x, y,color = treatment)) + 
  geom_point(size = 4 ) + 
  # geom_vline(xintercept = 2,
  #            color = 'red') + 
  # geom_vline(xintercept = 5,
  #            color = 'blue') + 
  theme_bw() + 
  labs(
    title = "Linearly Related variables with unequal Group Variances",
    x = "X",
    y = "Y",
    color = "Groups"
  ) + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  # Here comes the gganimate code
  transition_states(
    column_label,
    transition_length = 1
  ) +
  enter_fade() + 
  exit_shrink() +
  ease_aes('sine-in-out')


animate(p, fps=1,renderer = gifski_renderer())

```

</center>

We have two groups; the red points you will notice form a thinner band compared to the blue points which are likely to be more spread out. This is the consequence of the unequal variances of the two groups.

We have two different location effects at work here. One due to the binary variable represented by the blue and red points while the other is a continuous variable represented on the axis. There is one scale effect where the variability differs across the binary variable only; however you can imagine that we could have also considered the variability changing across the continuous variable that would result in patterns such as the funnel/cone shape where the variability increases as the continuous variable magnitude increases.

For those interested in a model:

$$
Y_i \sim N(\theta_i,\sigma_i) \ \  \ \ \ for \ i \ from \ 1 \ to \ n
$$

$$
\theta_i =  \beta_1 + \beta_2*Variable_{group_i} + \beta_3*Variable_{continuous_i}  
$$

$$
\sigma_i = \beta_4 + \beta_5*Variable_{group_i}  
$$

Now comes the variation that will be introduced across the location and scale parameters. Below are the graphs that give the range of the estimates used to create the datasets. We need a range for the $\beta_2,\beta_3$ (location effects) and $\beta_5$ (scale effect) as these are the effect sizes that we will try to recover. For each simulation one value will be randomly chosen from each of the below distributions. By simulating thousands of cases, we will explore the entire range of the parameter space of possibilities. Again this information will be sourced from a combination of your literature review and your own subject matter knowledge about what may be happening.

```{r fig.align='center',fig.cap= 'Distributions for Beta Effect Sizes'}
readRDS(
  paste0(file_path,"parameter_explore.rds")
)

```

For the continuous effect and group location effects we simulate both from normal distributions that have means 0.6 and 0.5 respectively and variances 0.2 and 0.1 respectively. The values for the difference in variance across the groups were simulated from a gamma distribution with mean and variance of 1. All of the distributions were truncated as shown in the graph.

## Criteria Checking

### Initial Range

The purpose of the line graphs shown below is to detail the information for a range of sample sizes for further inspection. This initial look is not to draw any hard conclusions but instead provide the insight as to what sample sizes we should be focusing in on.

The ability to detect the location effect for the grouping variable is first shown.

```{r}

readRDS(
   paste0(file_path,"grouptwo_location_detect_all_info")
)


```

For almost all samples we have a detection probability that is above 0.9; we do not have to worry much as to whether we will recover the population parameter within the credible intervals. Notice that the 1-2 allocation maintains an almost steady detection probability of at least 0.95.

However when we consider the precision, the noise around the estimate may give us quite large credible intervals thus one may want to collect a larger sample to gain a high enough level of precision. The next graph shows us how the interval length of the location effect changes across the different sample sizes.

```{r}
readRDS(  
    paste0(file_path,"grouptwo_location_all_info.rds")
)

```

```{r}

all_info_line_12 <- readRDS(  
    paste0(file_path,"all_info_line_12.rds")
)

all_length <- 1

all_info_sample_size_allocation_12 <- 3
while (all_length > 0.7) {
all_length <- predict(all_info_line_12,newdata = data.frame(sample_size = all_info_sample_size_allocation_12))
all_info_sample_size_allocation_12 = all_info_sample_size_allocation_12 +3  
}

```

The problem with the smaller sample sizes is that those tend to produce large interval lengths that may allocate a considerable amount of probability to regions which we know is quite far from the true effect size as we planned it. An interval length of 0.7 will be the criteria for this exercise. You may of course require much greater precision for your own purposes but as you can see this will require quite large sample sizes.

The good news is for this problem the 1-2 allocation is just as good (better if zoomed in although the difference may be negligible) as the 1-1 allocation at achieving this level of precision. This is because group one has a typical variance of one, while group two has a typical variance of two; and so allocating twice as many persons to group two helps to more efficiently capture the effects of the simulation. This may help with costs as it may be cheaper to sample from the group which requires the two members while only needing the one member from the more expensive group. This kind of detail is dependent on your situation.

For this problem we will consider the 1-2 allocation; it would seem that we should investigate the sample sizes of at roughly `r all_info_sample_size_allocation_12` to achieve our goal.

The next two graphs describe what happens with the scale effect size for the group variable. This effect size is responsible for the difference in variability noted earlier in the animation where the blue points were varying about its regression line more than the red points. Similar considerations as to the detection and interval length are considered for this problem.

<center>

```{r}

readRDS(
    paste0(file_path,"grouptwo_scale_detect_all_info")
)



```

```{r}

readRDS(
   paste0(file_path,"grouptwo_scale_all_info")
)

```

</center>

It should be noted that it seems to require a much smaller sample to pick up the scale effect size. This is because it is actually quite larger than the location effect size. The mean of the gamma distribution it is simulated from is one, thus is twice as large. It makes sense that a larger effect requires a much smaller sample size to detect.

It also requires a smaller sample size to achieve the 0.7 interval length for the scale effect size.

The sample size requirement has not increased. A smaller sample size of about 190 may capture the group scale effect but it would not capture the group location effect. Thus we still maintain the minimum sample size needed.

Finally for the initial set we consider the continuous variable location effect.

<center>

```{r}

readRDS(
    paste0(file_path,"grouptwo_X_location_detect_all_info.rds")
)

```

```{r}
readRDS(
    paste0(file_path,"grouptwo_X_location_all_info.rds")
)


```

</center>

What is interesting about continuous effect sizes, is that even though the size isn't that much larger than the group effect size, small sample sizes are able to get really precise intervals on the estimate. The variability of the betas are directly affected by how much variation is covered in the predictors; the larger the variation the better the estimates. This is basically saying that if you are able to collect information from a broad range of your continuous variable then this means you have more information contained in your predictor variable and this is reflected in the computation of your continuous effect size.

In the simulations we have assumed a random sample was taken over some arbitrary range. You should take another look at the animation and notice that we always seems to have values spread out across a large range for the x axis.

### Narrowed Range

The current sample size range will easily satisfy the detection and they have a good mean interval length. At this point the remainder of the criteria are computed and are available by hovering over the interval range for any sample size.

```{r}
readRDS(
  paste0(file_path,"samples_set_treatment_location.rds")
)

```

```{r}

graph_samples_info <- 
  readRDS(
     paste0(file_path,'graph_samples_info.rds')
  )

lower_range_cover_percent <- 
  graph_samples_info %>% 
  filter(
    sample_size < 375
  ) %>% 
  pull(treatmentgrouptwo_cover) %>% 
  median %>% 
  round(digits = 2) %>% 
  prod(100)


sample_cover_80 <- graph_samples_info %>% 
  filter(
    treatmentgrouptwo_cover >= 0.8
  ) %>% 
  pull(sample_size) %>% 
  min()


sample_above_70 <-  graph_samples_info %>% 
  filter(
    treatmentgrouptwo_above >= 0.7
  ) %>% 
  pull(sample_size) %>% 
  min()



```

Now here comes another level of protection you can offer yourself when it comes to estimate stability. With a particular sample size, you can have an average length across all the simulations that satisfy our needs, but just how often is this length criteria satisfied? Sample Sizes that are around our minimum initial range seems to satisfy our 0.7 requirement roughly `r lower_range_cover_percent`% of the time.

This leads to another criteria, which is to say how often do we want an interval length of 0.7 to be satisfied. Before rushing to say always, really should have the resources to needed to guarantee this. What resources are you willing to expend to get the results your want to your satisfactory level of certainty. In other words, how sure do you want to be about these estimates before you are willing to act on the information.

If you want the interval length to be satisfied lets say 80% of the time then that takes our sample size to roughly `r sample_cover_80`.

And yet there is another level of validity to consider. We know that on average our effect size should be be about 0.5 so we may insist that our credible intervals be above a certain number lets say 0.1 roughly speaking. In order to get that level of precision 70% of the time that would take our sample up to `r sample_above_70`.

For the rest of our exercise we will restrict the criteria to the interval length being less than or equal to 0.7 for roughly 80% of the simulations.

The following two graphs give the criteria for the group scale effect and the continuous location effect, all of which are already satisfied by the minimum sample size chosen for the group location effect.

```{r}
readRDS(
  paste0(file_path,"samples_set_sigma_treatment_scale.rds")
)

```

```{r}
readRDS(
  paste0(file_path,"samples_set_x_location.rds")
)

```

### Where does the criteria breakdown occur

Now that we have a sample size estimate that works for our needs. We now want to understand just what are the situations that occur which this sample size would not be able to handle.

The first graph below shows what happens across combinations of the group locations for groups one and two.

<center>

```{r}

readRDS(
   paste0(file_path,"breakdown_location.rds")
)

```

</center>

We have the baseline mean which is the mean for group one on the x axis, while on the y axis we have the effect size that we are trying to detect. It seems that for the roughly 20% of cases that this sample size fails for, it does not have anything to do with the magnitudes of the effect sizes as it seems to fail randomly.

The following graph gives a different picture where the effect sizes for the variances are concerned.

<center>

```{r}

readRDS(
   paste0(file_path,"breakdown_scale.rds")
)
```

</center>

And here we have a base level understanding of what is important in your experiment, and is especially important information for you to retrieve from your literature review.

There is a pattern to how the simulations fail with respect to how much variation there is in the simulation. There also seems to be an interaction type relationship where the failure rate of goes up depending on the levels of the sigma magnitude. This is to say that if you were to select a magnitude for the baseline sigma, the failure rate would depend on the corresponding magnitude of the sigma effect size.

It seems that variation that exceeds a sum total of roughly 2.2 is where the simulations start to fail. So for this experiment, as long as the total variation does not exceed this amount you have a good assurance that the sample size will perform as expected.

## Conclusion

Monte Carlo Simulations allow us to test a wide variety of conditions that one may want a chosen sample size to offer a good level of assurance for. This is technique like any other for Sample Size computation is highly dependent on information sourced from the literature, most important so the measures of variation.

I would encourage researchers to pay special attention when reporting estimates of variability in your own research as this will aid others in formulating their own procedure for data collection and costs.
