---
title: "empty thoughts"
description: |
  A short description of the post.
author:
  - name: Gavin Gordon
    url: https://example.com/norajones
date: 2021-12-04
output:
  distill::distill_article:
    self_contained: false
draft: true

bibliography: 'C:/Users/gavin/Documents/GitHub/myBlog/documents/biblio.bib'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

What do we use the covaraince of the beta parameters for. I didnt know we had those outside of bayesian. Used for simulations. Like how do they come into play? How can you look at the tables and say something about them without reference to the covariance of the estimates yikes.

Gotta get comfy talking about residuals. As well as isolating residuals and seeing where the model is going wrong. I dont like the idea of a standardized residual. Seems less informative.

Essay ideas

1.  Popular metrics: r square,our cross validation metrics for model evaluation

2.  Residuals: standard residuals, sum of residuals, mean grouping of residuals? What can this tell me about the left over variation? How well are my predictions doing? A large standard error doesnt say the variable isn't useful or bad. Standard errors are proportional to the variance in the population (assuming you have a variance parameter or whatever you use). So it can still be used to talk about how precise the estimate for the betas are but its in proportion not a direct quantification. Apparantly this approximates $\sigma$ as well chapter three gelman hierarchcical. The plots basically say, for the info contain in the variable, not all was captured properly by the model and there is still some left in the unexplained variation, talking about if there are patterns.

3.  stat significance: the precision with which your data estimates something. Something can not be measured to the precision that you want but still plays a valid role in the process. You gotta figure out why you cant measure this thing with the level of precision that you want. Can still include in the model for the mean while.

4.  How far can we stretch the latent variable formulation?

5.  Cross validation metrics: WAIC, LOO; batch CV; non-parametric counterparts?

6.  Relationship between the poisson, binomial, exponential - members of the exponential family that seem to be geared for dealing with different things. But kinda closes off some perspective. Need to branch out and see what other distributions handle other issues. So can we convert a binomial application for the logistic regression to a bernoulli application? Seems like the bernoilli case is just when you expand the case counts for the binomial case. I dont expect the inferences to be better; do you get better precision on the bernoulli? If so wtf then it just comes down to how many counts of data you are able to enter into the golem even if they really should have the same value?? Overdispersion can't occur for the binary/bournulli case, cause its one item/record per group. so the number of groups would be n. While the number of groups for the binomial would be whatever grouping category the bernoulli cases is grouped under. Feeling that heteorgenous model doesnt target the same thing as overdispersion, caues i expect the introduction of variance parameter to improve things with th binary logistic, yet overdispersion cant occur for this one. Idk cant wrap my head around this one. Now they're saying ye you can still do overdispersion but you introduce the grouping back in a multilevel mod style. This stuff is just 15 fucking ways of doing the same thing na. Wonder what the benefits are.

7.  How to know what a distribution is good for. Is there something in the formula of a poisson model that makes it good for counting, or it historically the case that we use it for counting. Then you have something like the logistic-binomial

8.  Distribution History Channel

Simulate data and recover features

-cdf

-pdf

moments 1-5

poisson

binomial

negative binomial

10. How to reparamaterize distributions to make them more interpretable im guessing. Take for instance the poisson distribution. This has an offset. Estimating it doesnt seemmmmm to change anything about the model. It just helps interpret the mean in terms of some standard. Not even sure you estimate the offset. So I guess you can do that for other distributions. The thing is after you remove the offset, you model the remainder with the linear part. Wonder how this all works out. The offset doesnt have to come from the data. It can be an external reference to something. In this example with the use of the police arrest rate, the offset was chosen to be the arrest rate from the last year. So i guess you get the difference from the offset, and the model that using the linear equation. So this answers a question, we are interested in the change from last year, what has been driving that change. Gotta see how they wrote the model for this. And they also saying you can let the data estimate the thing, so that it would fit better? as in relative to what, choosing it yourself or does this actually improve the fit in some way? @gelman2006data Is this a way of doing paired tests btw? Not sure if this will be reflected in the variability though.

11. When to have a multinomial logistic regression. When should you model it as such vs using modeing the categoies separately. First reason put forwards is when different factors go into determining each component then do it separately. Apparantly accoring to the latent variable formulation it might really be one line created by the z that gets cut regardless of ordered or unordered. @gelman2006data. So its saying that you'll be adding noisy fluff to what is going on if the predictor is useful to some cases but not others.

12. modeling variability. Through overdispersed models. Or models that have model the variance parameter explicittly. What be the difference. Overdisperase seems to add a paramter to capture a constant variance, while the other one lets you model it linearly or however else.

13. coding schemas. have a model interpretation repository; only for those that have problematic interpretations. Or just get a bunch of standardized interpretations. I guess.

exponential, gamma, and Weibull models for waiting-time data, and hazard models for survival data.

nonparametric models including generalized additive models, neural networks, and

13. some kind of choice models. using utility functions. This seems more hands on. Takes data directly from the people who you wanna know and models for those persons. Has ma attention. Well always has tbh going back to conjoint analysis. Can get an idea of how persons are viewing risk based on the mapping that works. For example that arsenic example where, a log fits better than linear, so people are viewing some ranges are more safe that others or something to that extent @gelman2006data. Generally seems like you have $P(benefit > risk)$ is what you're trying to find, where you have some expression for the benefit and another for the risk. It gives the impression that giving each household its own intercept is what makes it the individual level. But what if i gave group level characteritics its one intercepts. OR is there something i'm misunderstanding here; as that was the thing was supposed to stop having to consider what kind of error something from that book @kaltenbach2021statistical

14. The art of simulations. When use cases are complex enough that it warrants. So they say for predictive use summarazing info. we can use simulation to capture both predictive uncertainty (the error term in the regression model) and inferential uncertainty (the standard errors of the coefficients and uncertainty about the residual error) @gelman2006data . Simulation will also be crucial when working with nonlinear models such as logistic regression

15. How is bias affected by over/under representation?

16. issues with matching using propensity scores. Matching algorithms in general i guess. Matching with replacement. Matching without repacement @gelman2006data. Model misspefication for the scoring algorithm

17. Finding weights for data. Use of propensity score @gelman2006data. Advice it to use robust or non-parametric methods to create the weights. Apparantly the use of the scores to sub for the rest of the covariates. Although the reduction to that one dimension may have issues.

18. check out other bayesian estimation frameworks.
